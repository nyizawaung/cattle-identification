{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.structures import Boxes, pairwise_iou\n",
    "\n",
    "\n",
    "# Step 1: Register your test dataset\n",
    "camNo = \"Cam604\"\n",
    "data_path = r\"C:\\Users\\choni\\Pictures\\Merged image\"\n",
    "print(data_path)\n",
    "annotation_file = f\"{data_path}\\\\{camNo}_dec_16_coco.json\" # COCO-format JSON file\n",
    "image_dir = f\"{data_path}\\\\images\\\\\"  # Directory containing test images\n",
    "print(annotation_file)\n",
    "register_coco_instances(\"test_dataset_name\", {}, annotation_file, image_dir)\n",
    "\n",
    "\n",
    "# Step 2: Load configuration and model\n",
    "\n",
    "cfg = get_cfg()\n",
    "model_path = \"models\\\\March 2025\\\\Feb_March_Day_Night_15000_iters_v2\" \n",
    "                                                                      \n",
    "cfg.merge_from_file(f\"{model_path}/config.yml\")  # Path to config file\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6 #0.6\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.4 #0.3\n",
    "cfg.MODEL.WEIGHTS = f\"{model_path}/model_best.pth\"  # Path to trained model weights\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Step 3: Load the dataset and metadata\n",
    "dataset_dicts = DatasetCatalog.get(\"test_dataset_name\")\n",
    "metadata = MetadataCatalog.get(\"test_dataset_name\")\n",
    "\n",
    "# Step 4: Define IoU threshold for matching predictions with ground truth\n",
    "iou_threshold = 0.3\n",
    "\n",
    "output_dir = f\"{data_path}/detections_removed_mask\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#E:\\Nyi Zaw Aung\\Sumiyoshi_annotation\\Testing Annotation\\16.12.2024\\Resting Area\\Cam602\\images\n",
    "#E:\\Nyi Zaw Aung\\Sumiyoshi_annotation\\Testing Annotation\\16.12.2024\\Resting Area\\Cam602\\images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_precision_recall_accuracy_in_csv(precision, recall, accuracy, output_dir):\n",
    "    with open(f\"{output_dir}/{camNo}.csv\", \"w\") as f:\n",
    "        f.write(\"Precision, Recall, Accuracy\\n\")\n",
    "        f.write(f\"{precision}, {recall}, {accuracy}\\n\")\n",
    "        \n",
    "        \n",
    "def save_precision_recall_accuracy_APs_in_csv(precision, recall, accuracy, results,total_tp,total_fp,total_fn,total_gt,total_pred, output_dir):\n",
    "    segms = results.get(\"segm\",{})\n",
    "    with open(f\"{output_dir}/{camNo}.csv\", \"w\") as f:\n",
    "        f.write(\"Precision, Recall, Accuracy, TP,FP,FN,GT_total,Pred_total,AP, AP50, AP75\\n\")\n",
    "        f.write(f\"{precision}, {recall}, {accuracy},{total_tp},{total_fp},{total_fn},{total_gt},{total_pred}, {segms.get(\"AP\",None)} , {segms.get(\"AP50\",None)}, {segms.get(\"AP75\",None)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize counters for TP, FP, FN\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "total_gt = 0\n",
    "total_pred = 0\n",
    "for d in dataset_dicts:\n",
    "    img_path = d[\"file_name\"]\n",
    "    img = cv2.imread(img_path)\n",
    "    #print(img_path)\n",
    "    # Skip invalid or non-JPG files\n",
    "    if img is None or \".jpg\" not in d[\"file_name\"]:\n",
    "        continue\n",
    "    \n",
    "    # Get ground truth annotations (bounding boxes)\n",
    "    gt_boxes = [ann[\"bbox\"] for ann in d[\"annotations\"] if \"bbox\" in ann]\n",
    "    gt_boxes = Boxes(torch.tensor(gt_boxes)) if len(gt_boxes) > 0 else Boxes(torch.empty(0, 4))  # Handle empty cases\n",
    "    # Get predictions\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    \n",
    "    pred_boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n",
    "    \n",
    "    # Count detections and ground truth boxes\n",
    "    num_predictions = len(pred_boxes) if pred_boxes is not None else 0\n",
    "    num_ground_truths = len(gt_boxes)\n",
    "    total_gt += num_ground_truths\n",
    "    total_pred += num_predictions\n",
    "    # Compare predictions with ground truth\n",
    "    bad_flag = False  # Flag to mark if the image is \"bad\"\n",
    "    has_iou = True\n",
    "    if pred_boxes is not None:\n",
    "        iou_matrix = pairwise_iou(pred_boxes, gt_boxes) if num_ground_truths > 0 else torch.zeros((num_predictions, 0))\n",
    "        \n",
    "        # Match predictions to ground truth using IoU threshold (e.g., 0.5)\n",
    "        iou_threshold = 0.5\n",
    "        matched_gt_indices = set()\n",
    "        #print(img_path)\n",
    "        \n",
    "        if iou_matrix.numel() == 0:\n",
    "            has_iou = False\n",
    "        else:\n",
    "            for pred_idx in range(iou_matrix.shape[0]):\n",
    "                max_iou_idx = iou_matrix[pred_idx].argmax()\n",
    "                max_iou = iou_matrix[pred_idx][max_iou_idx] if num_ground_truths > 0 else 0\n",
    "                \n",
    "                if max_iou >= iou_threshold:\n",
    "                    matched_gt_indices.add(max_iou_idx)\n",
    "            \n",
    "            # Check for missing or extra detections\n",
    "            unmatched_gt_count = num_ground_truths - len(matched_gt_indices)  # Ground truths not matched by any prediction\n",
    "            unmatched_pred_count = num_predictions - len(matched_gt_indices)  # Predictions not matched to any ground truth\n",
    "            \n",
    "            if unmatched_gt_count > 0 or unmatched_pred_count > 0:\n",
    "                bad_flag = True\n",
    "    \n",
    "    bad_flag = num_ground_truths != num_predictions\n",
    "    if num_predictions > num_ground_truths :\n",
    "        total_fp += num_predictions - num_ground_truths\n",
    "        total_tp += num_ground_truths\n",
    "    elif num_ground_truths > num_predictions:\n",
    "        total_fn += num_ground_truths - num_predictions\n",
    "        total_tp += num_predictions\n",
    "    else:\n",
    "        total_tp += num_predictions\n",
    "        \n",
    "    \n",
    "    #elif num_ground_truths > 0:\n",
    "    #    bad_flag = True  # If there are no predictions but ground truths exist\n",
    "    \n",
    "    #elif num_predictions > 0:\n",
    "    #    bad_flag = True  # If there are predictions but no ground truths\n",
    "    \n",
    "    # Visualize predictions (bounding boxes and masks)\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1)  # Use grayscale for better visibility\n",
    "    out = v.draw_instance_predictions(instances)\n",
    "    \n",
    "    # Save the image with predictions and add \"bad\" to the file name if applicable\n",
    "    base_filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    suffix = f\"_det{num_predictions}_gt{num_ground_truths}\"\n",
    "    \n",
    "    if bad_flag:\n",
    "        suffix += \"_bad\"\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{base_filename}{suffix}.jpg\")\n",
    "    #if num_ground_truths > 0 or num_predictions > 0:\n",
    "    cv2.imwrite(output_path, out.get_image()[:, :, ::-1])\n",
    "\n",
    "print(\"Processing complete! All images have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = total_tp / (total_tp + total_fp) \n",
    "recall = total_tp / (total_tp + total_fn)\n",
    "accuracy = total_tp / (total_tp + total_fp + total_fn)\n",
    "#total_tp = 0\n",
    "#total_fp = 0\n",
    "#total_fn = 0\n",
    "#total_gt = 0\n",
    "#total_pred = 0\n",
    "print(f\"Precision, {precision}, accuracy {recall}, accuracy {accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator and test loader\n",
    "evaluator = COCOEvaluator(\"test_dataset_name\", cfg, False, output_dir=output_dir)\n",
    "val_loader = build_detection_test_loader(cfg, \"test_dataset_name\")\n",
    "\n",
    "# Perform evaluation\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "#print(results)  # This will include precision and recall metrics\n",
    "\n",
    "save_precision_recall_accuracy_APs_in_csv(precision, recall, accuracy,results, total_tp,total_fp,total_fn,total_gt,total_pred,output_dir)\n",
    "\n",
    "if False:\n",
    "    for d in dataset_dicts:\n",
    "        img_path = d[\"file_name\"]\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Skip invalid or non-JPG files\n",
    "        if img is None or \".jpg\" not in d[\"file_name\"]:\n",
    "            continue\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = predictor(img)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "        \n",
    "        # Visualize predictions (including masks)\n",
    "        v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE_BW)  # Use grayscale for better visibility\n",
    "        out = v.draw_instance_predictions(instances)\n",
    "        \n",
    "        # Draw yellow masks manually\n",
    "        if instances.has(\"pred_masks\"):\n",
    "            pred_masks = instances.pred_masks.numpy()  # Shape: (N, H, W), where N is the number of detected objects\n",
    "            \n",
    "            for mask in pred_masks:\n",
    "                # Create a yellow overlay for the mask\n",
    "                yellow_mask = np.zeros_like(img, dtype=np.uint8)\n",
    "                yellow_mask[:, :, 1] = 255  # Set green channel to max (R=0, G=255, B=0 for yellow)\n",
    "                yellow_mask[:, :, 2] = 255  # Set red channel to max (R=255, G=255, B=0 for yellow)\n",
    "\n",
    "                # Blend the mask with the original image\n",
    "                mask_indices = mask.astype(bool)  # Convert binary mask to boolean for indexing\n",
    "                img[mask_indices] = cv2.addWeighted(img, 0.5, yellow_mask, 0.5, 0)[mask_indices]\n",
    "        \n",
    "        # Save the image with predictions and yellow masks\n",
    "        output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
    "        cv2.imwrite(output_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replace with your dataset paths\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clone_from_super_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

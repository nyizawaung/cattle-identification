import timm
import torch, detectron2
import torch.nn as nn
import torch.optim as optim
#import matplotlib.pyplot as plt
from detectron2.utils.logger import setup_logger
#import asyncio
#from defisheye import Defisheye
#import threading
#import pickle
#import torchvision.models as models
#from torchvision import transforms
import torch.nn.functional as F
from typing import Iterable, List, NamedTuple

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.config import CfgNode, get_cfg
from detectron2.modeling import build_model
from detectron2.structures import Instances

from sklearn.metrics.pairwise import cosine_similarity

import detectron2.data.transforms as T
#from torchvision.models import EfficientNet_V2_M_Weights
import cv2
import numpy as np
import os
from helpers.helper import Helper
from pathlib import Path
#import time
#from ANNModel import ANN
from PIL import Image
from torchvision import datasets, transforms, models
#import torch.nn.functional as F
#from efficientNet import Efficient_Net as EN
from datetime import datetime
from collections import Counter
import csv
from PIL import Image
import math
from enum import Enum
import pandas as pd
import json
from datetime import datetime
import heapq
import itertools
from torch.utils.data import DataLoader, Dataset
from numpy import ndarray
from collections import defaultdict


class CheckSimilarity():
    #def __init__(self):
    #EfficientNetV2-S (pretrained, without classification head)
    #effnet_model = timm.create_model('tf_efficientnetv2_s', pretrained=True, num_classes=0)
    model = timm.create_model('resnet101', pretrained=True, num_classes=0)
    model.eval()

    #Move to GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    #Preprocessing
    preprocess = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])

    def extract_embedding(self,img: Image.Image) -> np.ndarray:
        tensor = self.preprocess(img).unsqueeze(0).to(self.device)
        with torch.no_grad():
            embedding = self.model(tensor)
        return embedding.squeeze().cpu().numpy()

    def cosine_similarity_search(self,last_image_paths: list[str],current_image: Image.Image): #current_image: Image.Image):
        # Extract query embedding
        query_embedding = self.extract_embedding(current_image)

        db_embeddings = []
        valid_paths = []

        for path in last_image_paths:
            try:
                img = Image.open(path).convert("RGB")
                emb = self.extract_embedding(img)
                db_embeddings.append(emb)
                valid_paths.append(path)
            except Exception as e:
                print(f"Error loading {path}: {e}")

        if not db_embeddings:
            return None, 0.0

        # Calculate cosine similarity
        db_embeddings_np = np.array(db_embeddings)
        similarities = cosine_similarity([query_embedding], db_embeddings_np)[0]

        best_idx = np.argmax(similarities)
        best_score = similarities[best_idx]
        best_match_path = valid_paths[best_idx]

        return best_match_path, best_score
class MASKLOCATION():
    def __init__(self, tracking_id, mask, cam_counter, current_point, is_touching_bottom=False, is_touching_top=False):
        self.tracking_id = tracking_id
        self.mask = mask
        self.is_touching_bottom = is_touching_bottom
        self.is_touching_top = is_touching_top
        self.cam_counter = cam_counter
        self.current_point = current_point #centroid
        self.last_mask_area = np.sum(mask)
        self.mask_area = np.sum(mask)

class CoreProcessResponseModel():
    def __init__(self, cam_counter, boxes,tracked_indexes, tracked_ids, predicted_ids, original_predicted_ids, colored_mask ):
        self.cam_counter = cam_counter
        self.boxes = boxes
        self.tracked_indexes = tracked_indexes
        self.tracked_ids = tracked_ids
        self.predicted_ids = predicted_ids
        self.original_predicted_ids = original_predicted_ids
        self.colored_mask = colored_mask
        
    
        # response_model = {
        #     "cam_counter":cam_counter,
        #     "tracked_indexes": tracked_indexes,
        #     "tracked_ids": tracked_ids,
        #     "predicted_ids": predicted_ids,
        #     "original_predicted_ids": original_predicted_ids,
        #     "colored_mask": colored_mask
        # }
    
class Prediction(NamedTuple):
    x: float
    y: float
    width: float
    height: float
    score: float
    class_name: str

images = []
class ImageDataset(Dataset):

    def __init__(self, imagery: List[Path]):
        self.imagery = imagery

    def __getitem__(self, index) -> ndarray:
        return cv2.imread(self.imagery[index])

    def __len__(self):
        return len(self.imagery)


class TRACKING_TYPE(Enum):
    BOX = 'BOX'
    CENTROID = 'CENTROID'
    IOU = 'IOU'


class BatchPredictor:
    def __init__(self, cfg: CfgNode, batch_size: int, workers: int):
        self.cfg = cfg.clone()  # cfg can be modified by model
        self.batch_size = batch_size
        self.workers = workers
        self.model = build_model(self.cfg)
        self.model.eval()

        checkpointer = DetectionCheckpointer(self.model)
        checkpointer.load(cfg.MODEL.WEIGHTS)

        self.aug = T.ResizeShortestEdge(
            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],
            cfg.INPUT.MAX_SIZE_TEST
        )

        self.input_format = cfg.INPUT.FORMAT
        assert self.input_format in ["RGB", "BGR"], self.input_format

    def __collate(self, batch):
        data = []
        for image in batch:
            # Apply pre-processing to image.
            if self.input_format == "RGB":
                # whether the model expects BGR inputs or RGB
                image = image[:, :, ::-1]
            height, width = image.shape[:2]

            #image = self.aug.get_transform(image).apply_image(image)
            image = image.astype("float32").transpose(2, 0, 1)
            image = torch.as_tensor(image)
            data.append({"image": image, "height": height, "width": width})
        return data

    def __call__(self, imagery: images) -> Iterable[List[Prediction]]:
        """[summary]

        :param imagery: [description]
        :type imagery: List[Path]
        :yield: Predictions for each image
        :rtype: [type]
        """
        dataset = images
        loader = DataLoader(
            dataset,
            self.batch_size,
            shuffle=False,
            num_workers=self.workers,
            collate_fn=self.__collate,
            pin_memory=True
        )

        with torch.no_grad():
            for batch in loader:
                results: List[Instances] = self.model(batch)
                #yield from [self.__map_predictions(result['instances']) for result in results]
                yield from results

    def __map_predictions(self, instances: Instances):
        instance_predictions = zip(
            instances.get('pred_boxes'),
            instances.get('pred_masks')
        )

        predictions = []
        for box, score, class_index in instance_predictions:
            x1 = box[0].item()
            y1 = box[1].item()
            x2 = box[2].item()
            y2 = box[3].item()
            width = x2 - x1
            height = y2 - y1
            prediction = Prediction(
                x1, y1, width, height, score.item())
            predictions.append(prediction)
        return predictions

class CorrectDistortedImage:
    def createSetting(self, k1,k2,k3,p1,p2,focal_length_x,focal_length_y,center_x,center_y,scale,aspect):
            # self.root = root
            # self.root.title("Enhanced Barrel Distortion Correction")
            # self.root.geometry("1200x800")  # Larger window for both images and controls

            # Initialize parameters
            self.image = None
            self.original_image = None 

            #5-52 template
            # self.k1 = -0.08  # primary radial distortion coefficient
            # self.k2 = 0.0  # secondary radial distortion coefficient
            # self.k3 = 0.0  # tertiary radial distortion coefficient
            # self.p1 = 0.0  # tangential distortion coefficient
            # self.p2 = 0.0  # tangential distortion coefficient
            # self.focal_length_x = 1.0  # focal length x-axis
            # self.focal_length_y = 1.0  # focal length y-axis
            # self.center_x = 0.43  # center x (relative to width)
            # self.center_y = 0.02  # center y (relative to height)
            # self.scale = 1.01  # scale factor for the corrected image
            # self.aspect = 1.0  # aspect ratio adjustment

            # self.k1 = 0.03  # primary radial distortion coefficient
            # self.k2 = 0.0  # secondary radial distortion coefficient
            # self.k3 = 0.0  # tertiary radial distortion coefficient
            # self.p1 = 0.0  # tangential distortion coefficient
            # self.p2 = 0.0  # tangential distortion coefficient
            # self.focal_length_x = 1.0  # focal length x-axis
            # self.focal_length_y = 1.0  # focal length y-axis
            # self.center_x = 0.65  # center x (relative to width)
            # self.center_y = 0.0  # center y (relative to height)
            # self.scale = 1.01  # scale factor for the corrected image
            # self.aspect = 1.0  # aspect ratio adjustment
            setting = {
                'k1': k1,
                'k2': k2,
                'k3': k3,
                'p1': p1,
                'p2': p2,
                'focal_length_x': focal_length_x, 
                'focal_length_y': focal_length_y,
                'center_x': center_x, 
                'center_y': center_y,
                'scale': scale, 
                'aspect': aspect
            }
            return setting

    def apply_correction(self,image,settings):
            if image is None:
                return None
            #print(settings)
            # Get image dimensions
            height, width = image.shape[:2]
            
            # Calculate center point in pixels
            center_x = width * settings['center_x']
            center_y = height * settings['center_y']
            
            # Adjust focal length based on image dimensions
            focal_x = settings['focal_length_x'] * width
            focal_y = settings['focal_length_y'] * width * settings['aspect']
            
            # Create camera matrix
            camera_matrix = np.array([
                [focal_x, 0, center_x],
                [0, focal_y, center_y],
                [0, 0, 1]
            ], dtype=np.float32)
            
            # Create distortion coefficients array [k1, k2, p1, p2, k3]
            distortion_coeffs = np.array([settings['k1'], settings['k2'], settings['p1'], settings['p2'], settings['k3']], dtype=np.float32)
            
            # Get optimal new camera matrix
            new_camera_matrix, _ = cv2.getOptimalNewCameraMatrix(
                camera_matrix, distortion_coeffs, (width, height), 1, (width, height))
            
            # Scale the new camera matrix
            new_camera_matrix[0, 0] *= settings['scale']  # Scale fx
            new_camera_matrix[1, 1] *= settings['scale']  # Scale fy
            
            # Undistort the image
            undistorted_image = cv2.undistort(
                image, camera_matrix, distortion_coeffs, None, new_camera_matrix)
            
            undistorted_image = undistorted_image[15:height, 0:width]  # Crop the image to remove black borders
            return undistorted_image
    
    def show_image(self, image,window_name="CorrectedImage"):
            cv2.imshow(window_name, image)
            cv2.waitKey(0)
            cv2.destroyAllWindows()

    def save_CorretedImage(self, image, filename):
            if image is not None:
                cv2.imwrite(filename, image)
                print(f"Corrected image saved as {filename}")
            else:
                print("No image to save.")



class BatchFrameReader:
    def __init__(self, video_paths, batch_size=100):
        self.caps = []
        for video_path in video_paths:
            if not os.path.exists(video_path):
                raise ValueError(f"Video path {video_path} does not exist.")
            
            self.caps.append(cv2.VideoCapture(video_path))
        self.batch_size = batch_size
        self.buffer = []
        self.eof = False  # End of file flag
        self.total_frames = 0

    def _read_next_batch(self):
        
        cam_buffer = []
        is_quick
        while frames_read < self.batch_size and not self.eof:
            for i in range(len(self.caps)):
                success, frame = self.caps[i].read()
                if not success:
                    self.eof = True

                    break
                cam_buffer[i].append(frame)
            self.total_frams +=1
        for i in range(len(self.caps)):
            if len(cam_buffer[i]) > 0:
                self.buffer.extend(cam_buffer[i])
                self.total_frames += len(cam_buffer[i])
                cam_buffer[i] = []
        
    ## call this with threading
    def get_batch(self):
        if len(self.buffer) < self.batch_size and not self.eof:
            self._read_next_batch()

        batch = self.buffer[:self.batch_size]
        self.buffer = self.buffer[self.batch_size:]  # remove returned frames
        del self.buffer
        return batch

    def has_more(self):
        return len(self.buffer) > 0 or not self.eof

    def release(self):
        for cap in self.caps:
            cap.release()
        del self.caps[:]


#torch.cuda.empty_cache()
class CATTLE_IDENTIFICATION():
    def load_detector():
        # Other initialization code...
        
        # Pre-initialize the predictor once
        detection_model = 'models\\Base-sidelane July 2025\\Base_rtx8000_June_10000_v1/model_best.pth'
        model_config = 'models\\Base-sidelane July 2025\\Base_rtx8000_June_10000_v1/config.yml'
        
        cfg = get_cfg()
        cfg.merge_from_file(cfg_filename=model_config)
        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8 #0.6
        cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.4 #0.3
        
        cfg.MODEL.WEIGHTS = detection_model
        
        print("Initializing model")
        # Set device explicitly
        #self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return BatchPredictor(cfg, batch_size=8, workers=0)
        #print(self.predictor, ' is predictor')

    # Step 1: Set up data transformations
    data_transforms = {
        'train': transforms.Compose([
        #    transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'validation': transforms.Compose([
        #   transforms.Resize(256),
        #   transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }


    # Step 8: Prediction function
    def predict_stack_cattle_id(self, images, threshold=0.7):
        """
        Predict cattle IDs from a stack of images.
        
        Args:
            images: List of PIL Images or numpy arrays
            threshold: Confidence threshold for predictions
        
        Returns:
            List of [class_name, count] pairs for predictions above threshold
        """
        self.model.eval()

        # Preprocess all images
        #processed_images = [self.preprocess_image(img) for img in images]
        
        # Stack images into a single tensor
        image_tensor = torch.stack(images).to(self.device)

        with torch.no_grad():
            # Get model predictions
            outputs = self.model(image_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            max_probs, preds = torch.max(probabilities, 1)

            # Filter predictions based on threshold
            filtered_classes = [
                self.class_names[str(pred.item())] 
                for pred, prob in zip(preds, max_probs) 
                if prob.item() >= threshold
            ]

            # Count occurrences of filtered classes
            class_counts = Counter(filtered_classes)
            filtered_counts = [[key, count] for key, count in class_counts.items()]

        return filtered_counts

    def predict_single_image(self, image, return_probability=False):
        """
        Predict cattle ID for a single image.
        
        Args:
            image: PIL Image or numpy array
            return_probability: Whether to return the confidence score
        
        Returns:
            predicted class name (and probability if return_probability=True)
        """
        self.model.eval()
        
        # Preprocess image
        processed_image = self.preprocess_image(image)
        image_tensor = processed_image.unsqueeze(0).to(self.device)

        with torch.no_grad():
            output = self.model(image_tensor)
            probabilities = torch.softmax(output, dim=1)
            max_prob, pred = torch.max(probabilities, 1)

        predicted_class = self.class_mapping[pred.item()]
        
        if return_probability:
            return predicted_class, max_prob.item()
        return predicted_class
    
    def _load_model(classLen, model_path,device):
        # Create model architecture
        # model = timm.create_model("tf_efficientnetv2_s", pretrained=False, num_classes=classLen)
        # model.load_state_dict(torch.load(model_path, map_location=device))
        # model.eval()
        
        model = timm.create_model('convnextv2_base.fcmae_ft_in22k_in1k', pretrained=False, num_classes=classLen)
         # Load the trained weights
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)
        model.eval()
        
        
        return model.to(device)

    def _load_model_resnet(classLen, model_path,device):
        # Create model architecture
        model = timm.create_model('resnet101', pretrained=False)
        model.fc = nn.Sequential(
            nn.Linear(model.fc.in_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, classLen)
        )
        
        # Load trained weights
        model.load_state_dict(torch.load(model_path))
        model.eval()
        return model.to(device)
    
    def get_files_from_folder(self,path, limitX=20):
    # Get the list of files in the directory
        files = os.listdir(path)
        
        # Use heapq.nlargest to get the last 'limitX' largest numeric files without sorting everything
        largest_files = heapq.nlargest(limitX, files, key=lambda x: int(x.split('.')[0]))  # Assumes numeric filenames
        
        return np.asarray(largest_files)
    
    def get_files_from_folder_scan_sort(self,path, limitX=20):
    # Use os.scandir() for faster directory scanning
        with os.scandir(path) as entries:
        # Collect all filenames into a list
            files = [entry.name for entry in entries if entry.is_file()]
            
            # Sort the files based on the numeric part of the filenames (assuming filenames contain numbers)
            files.sort(key=lambda x: int(x.split('_')[0]), reverse=True)  # Adjust as needed for your filenames
            
            # Return the top 'limitX' files
            return files[:limitX]

    def batch_identification(self,save_dir,tracking_id,FORCE_MISSED_DETECTION = False,path = None):
        start_time = datetime.now()
        counter = 20
        save_path = path
        #global batch_size 
        if path is None:
            save_path = f"{save_dir}//{tracking_id}"
        batch_size = self.REIDENTIFY_MISSED_COUNT if not FORCE_MISSED_DETECTION else self.identification_batch_size
        #images = self.get_files_from_folder_scan_sort(save_path,batch_size)
        images = self.get_last_X_paths(tracking_id,batch_size)
        #print('images: for tracking_id ',tracking_id , " are ", images)
        stacked_images = []
        if images is None:
            return None
        for image in images:
            #if counter < 1:
                #break
            #img_path = os.path.join(save_path, image)
            frame = cv2.imread(image)
            #predict_cattle_id(frame)
            # Ensure the image is in RGB format
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Preprocess the image using the defined transforms
            input_tensor = self.data_transforms['validation'](frame).unsqueeze(0)  # Add batch dimension
            input_tensor = input_tensor.to(self.device)

            # Append the processed image tensor to the list
            stacked_images.append(input_tensor[0])  # Append the first element to keep it as (C, H, W)
            #counter -= 1
        
        # Call predict_stack_cattle_id with the stacked_images
        #return self.predict_stack_cattle_id(stacked_images)
        return self.predict_stack_cattle_id(stacked_images)
        #print("print total duration :", datetime.now()-start_time ) 
        #return 
    # Replace the path with your actual path
    #batch_size = 50
    #batch_identification("a",1,r"E:\Nyi Zaw Aung\Python\detectron2\Identification_dataset\Sumiyoshi-37\validation\M105")
    #batch_identification("a",1,r"E:\Nyi Zaw Aung\Python\detectron2\Identification_dataset\Random")
    def calculate_center_of_box(self,box):
        x1 = box[0]
        y1 = box[1]
        x2 = box[2]
        y2 = box[3]
        cx = int(x1) + int((x2-x1)/2)
        cy = int(y1) + int((y2-y1)/2)
        print(f"cx : {cx}, cy : {cy}")
        # match corner:
        #     case BoxCorner.CENTER: return (cx, cy)
        #     case BoxCorner.TOP_LEFT: return (x1, y1)
        #     case BoxCorner.TOP_RIGHT: return (x2, y1)
        #     case BoxCorner.BOTTOM_LEFT: return (x1, y2)
        #     case BoxCorner.BOTTOM_RIGHT: return (x2, y2) 

    #yellow = (0, 255, 255)
    #cam_10 = [(300,100), (550,100), (800,100), (1050,100), (1300,100), (1550,100),]

    def draw_circle(self,cv2, center_coordinate, image, color):
        cv2.circle(image, center_coordinate, radius=3, color=color, thickness=-1)
    
    def get_color(self,tracking_id):
        #get index of tracking_id
        try:
            idx = self.STORED_IDS.index(int(tracking_id))
            if idx<=4:
                color = self.colors[idx-1]
            else:
                idx = idx * 3
                color = ((37 * idx) % 255, (17 * idx) % 255, (29 * idx) % 255)
            return color
        except:
            return (0, 255, 0)  # Default color if tracking_id not found
    # Function to draw a bounding box and annotate the image
    def draw_bounding_box(self,image, box, label,tracking_id,font_scale = 2,color =(0, 255, 0)):
        # Extract the coordinates from the box
        x1, y1, x2, y2 = box
        #print(x1,' ',y1,x2,y2)

        # Draw the bounding box rectangle on the image
        #get_color(tracking_id)
        #cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
        cv2.rectangle(image, (x1, y1), (x2, y2), self.get_color(tracking_id), 2)
        #text = f'{tracking_id}'
        # Define the text properties
        if label == -1:
            label = '-1'
        elif label == 'Identifying' :
            label = f'Tracking_{tracking_id}'
        else:
            label = f'{label}_{tracking_id}'
        text = label
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        thickness = 2

        # Calculate the size of the text
        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)

        # Calculate center of the box
        center_x = x1 + (x2 - x1) // 2
        center_y = y1 + (y2 - y1) // 2

        # Adjust text position so it's centered
        text_x = center_x - text_width // 2
        text_y = center_y + text_height // 2

        
        # Put the label text on the image (no background)
        cv2.putText(image, text, (text_x, text_y), font, font_scale, color, thickness, cv2.LINE_AA)
        
        # tracking label
        #size down
        #diagonal = self.CALCULATE_EUCLIDEAN_DISTANCE((x1,y1),(x2,y2))
        
        #text = f'd_{diagonal:.2f}'
        # if label == 'Identifying' or True:
        #     text = f'{tracking_id}'
        #     #font_scale = font_scale 
        #     (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)
        #     text_x = x2 - text_width #right side
        #     text_y = y1 if y1 >= 20 else y1 + 10 + text_height #same height 
            
        #     cv2.rectangle(image, (text_x, text_y - text_height - 10), (text_x + text_width, text_y), color, -1)

        #     # Put the label text on the image
        #     cv2.putText(image, text, (text_x, text_y), font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)

    # Example usage
    def overlay(self,image, mask, color, alpha, resize=None):
        """Combines image and its segmentation mask into a single image.
        
        Params:
            image: Training image. np.ndarray,
            mask: Segmentation mask. np.ndarray,
            color: Color for segmentation mask rendering.  tuple[int, int, int] = (255, 0, 0)
            alpha: Segmentation mask's transparency. float = 0.5,
            resize: If provided, both image and its mask are resized before blending them together.
            tuple[int, int] = (1024, 1024))

        Returns:
            image_combined: The combined image. np.ndarray

        """
        # color = color[::-1]
        colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)
        colored_mask = np.moveaxis(colored_mask, 0, -1)
        masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)
        image_overlay = masked.filled()

        if resize is not None:
            image = cv2.resize(image.transpose(1, 2, 0), resize)
            image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)

        image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)

        return image_combined

    def SUMIYOSHI_ROI(self,x1,y1,x2,y2,h,w):
        #print(x1, '  ',y1, '  ',x2, '  ',y2, '  ',h, '  ',w)
        #if(x1<int(X1*(w/default)) or x2>int(X2*(w/default)) or y1<int(Y1_NEW*(h/default)) or y2>int(Y2_NEW*(h/default)) or x1>=int(X2*(w/default))):
        #    return False
        print (x2-x1,' is cow width', x2, ' - ', x1 )
        if y1 < 250 or y2 > 3600 : 
            print('I am outside of the ROI')
            return False
        #print(y1, " y1 and y2 ",y2) 
        if(y2 - y1>2400 or y2-y1<1000): #1400 to 700 Beforee
            print(y2-y1, ' I am too big')
            return False
        #if ( x1 < 45 * (w/429) or x2 > 500 * (w/429)): #measured on 429 x 724 size
        #    print(x1 ,'< ',50 * (w/429),' or ',x2 ,'>', 500 * (w/429))
        #    return False
        
        #if(x2-x1 > 500)
        return True 

    def HOKKAIDO_ROI(self,x1,y1,x2,y2,h,w):
        #print(x1, '  ',y1, '  ',x2, '  ',y2, '  ',h, '  ',w)
        
        X1=200 #same as NEW_BLACK_X1
        X2=400 #same as NEW_BLACK_X2 # incase of x2 out of bound
        
        Y1_NEW=50#125  #decrease here to extend, increase to shrink 
        Y2_NEW=630  #5
        default = 640
        X1
        if(x1<int(X1*(w/default)) or x2>int(X2*(w/default)) or y1<int(Y1_NEW*(h/default)) or y2>int(Y2_NEW*(h/default)) or x1>=int(X2*(w/default))):
            return False
        #if(y2 - y1>1500 or y2-y1<700): #1400 to 700 Before
        #    return False
        return True  

    def FilterSize(self,y1,y2,h25,area,h75,max_freq_pos):
        print('Filter size y2 and y1 :->',y2,' - ' ,y1 )
        #if y1 >= h25*1.5 and y2 <= h75 and (y2-y1<700 or area < 80000) : #within specific range # old camera
        if y1 >= h25*1.5 and y2 <= h75 and area < 80000 : #within specific range  # new camera
            print(y2-y1,' size skipped due to',y2,' - ' ,y1 )
            return False
        #elif y2>h75 and max_freq_pos>=50 and max_freq_pos<=100:
        #    return False
        return True

    def get_filename_from_file(self,file_path,ext):
        head,filename = os.path.split(file_path)
        filename = filename.split(ext)[0]
        return filename



    def current_datetime(self):
        current_datetime = datetime.now()
        now = current_datetime.strftime("%Y-%m-%d_%H-%M-%S")
        return str(now)

    def CalculateBoxArea(self,xyxy):
        w = xyxy[2] - xyxy[0] + 1
        h = xyxy[3] - xyxy[1] + 1 
        #print('width ',w,' height ',h)
        return w*h

    def CalculateIOU(self,prev,current):
        x1 = max(prev[0],current[0])
        y1 = max(prev[1],current[1])
        x2 = min(prev[2],current[2])
        y2 = min(prev[3],current[3])
        w = x2 - x1
        h = y2 - y1
        if w<=0 or h<=0:
            return 0
        area = max(0,x2-x1 +1) * max(0,y2-y1+1)
        #area = w * h
        #print('total area ',area)
        prev_area = self.CalculateBoxArea(prev)
        current_area = self.CalculateBoxArea(current)
        difference = abs(prev_area - current_area)
        #print('position 1',prev, 'position 2',current)
        #print('prev position, current position',prev_area,current_area)
        #return area / CalculateBoxArea(prev)
        #print('two box area ',prev_area,' ',current_area)
        prev_area = current_area if prev_area > current_area else prev_area
        
        if prev_area / 4 >= difference : # if size is not too different than previous
            return area/prev_area   
        return 0    
    #    iou = area / float(prev_area + current_area - area)
        
        #return io
        
    
    def IOU_Tracking(self,x1,y1,x2,y2):
        
        is_new = True
        current_id = -1
        global LAST_Y
        is_Moving = True
        if len(self.STORED_IDS)>0:
            LAST_Y = self.STORED_XYXY[-1][1] #x1y1x2y2
        
        #region dynamnic threshold
        dynamic_IOU = self.IOU_TH
        #middle = h*0.25
        if y1 < self.middle and False:
            if y1<(self.middle/2):
                dynamic_IOU += (1-self.IOU_TH) * ((self.middle-y1) / (self.middle-30))  #dynamic IOU threshold
            else : 
                dynamic_IOU += (1-self.IOU_TH) * ((self.middle-y1) / (self.middle))  #dynamic IOU threshold
            print('dynamic_threshold is :',dynamic_IOU)
        #endregion dynamic threshold
        closet = -1
        closet_iou = 0
        for i in range(len(self.STORED_IDS)):
            iou =self.CalculateIOU(self.STORED_XYXY[i],[x1,y1,x2,y2])
            #print(iou,' is iou percentage')
            #print(y1,' is current y1 position')
            if is_new and iou > dynamic_IOU :    
                is_new = False
                closet = i
                #continue
                break
                if iou > closet_iou:
                    closet_iou = iou
                    closet = i
        
        if closet>-1 :
            #print(closet, ' I am the chosen one!!!')
            self.STORED_XYXY[closet] = [x1,y1,x2,y2]
            current_id = str(self.STORED_IDS[closet])
            self.TOTAL_MISSED_COUNT += self.STORED_MISS[closet] - 1
            self.STORED_MISS[closet] = 1
            is_Moving = closet_iou < 90
                    
        if is_new:
            #if y1 < LAST_Y or y1 < middle+300: #new cattle is above the last cattle, means Id 5 is in front of 4. 
            #    print(y1 ,' is current y1 and last y1 is: ',LAST_Y,' middle is ',middle-300)
            #    return -1
            #print('Adding new cattle')
            self.CATTLE_LOCAL_ID+=1
            self.STORED_IDS.append(self.CATTLE_LOCAL_ID)
            self.STORED_MISS.append(1)
            self.STORED_XYXY.append([x1,y1,x2,y2])
            current_id = str(self.CATTLE_LOCAL_ID)
        
        
        
        return [current_id],is_Moving



    def CALCULATE_EUCLIDEAN_DISTANCE(self,prev_centroid, current_centroid):
        #print(prev_centroid)
        #print(current_centroid)
        x_sq = (prev_centroid[0] - current_centroid[0]) ** 2
        y_sq = (prev_centroid[1] - current_centroid[1]) ** 2
        distance = ( x_sq + y_sq ) ** 0.5
        #print(distance)
        return distance
    def Centroid_Tracking(self,mask,w,h,cam_counter,y1,y2):
        
        tracking_missed_frame = 1
        is_new = True
        current_id = -1
        if len(self.STORED_IDS)>0:
            LAST_Y = self.STORED_XYXY[-1][1] #x1y1x2y2
        closet = -1
        closet_iou = 0
        threshold = (w/2) if w<=h else (h/2)
        #threshold = threshold * 0.8
        threshold = threshold + 10 # is original
        current_point = self.find_mask_center(mask,cam_counter)
        least_missed_frame = 100
        for i in range(len(self.STORED_IDS)):
            distance = self.CALCULATE_EUCLIDEAN_DISTANCE(self.STORED_XYXY[i],current_point)
            
            if is_new and distance <= threshold :    
                #continue
        #       is_new = False
        #       closet = i
        #       break
                if distance < threshold/2: #distance is very close
                    is_new = False
                    closet = i    
                    #is_moving = distance > threshold / 3
                    
                    break
                elif distance > closet_iou:
                    closet_iou = distance
                    closet = i
                    
                #TORED_XYXY[-i] = [x1,y1,x2,y2]
                #current_id = str(STORED_IDS[-i])
                #TOTAL_MISSED_COUNT += STORED_MISS[-i] - 1
                #STORED_MISS[-i] = 1
        
        if closet>-1 :
            #print(closet, ' I am the chosen one!!!')
            self.STORED_XYXY[closet] = current_point
            current_id = str(self.STORED_IDS[closet])
            self.TOTAL_MISSED_COUNT += self.STORED_MISS[closet] - 1
            tracking_missed_frame = self.STORED_MISS[closet]-1
            #if tracking_missed_frame > 2:
            #    print(f"{current_id} has {tracking_missed_frame} missed frames")
            self.STORED_MISS[closet] = 1
            is_new = False

            #region update tracking
            #is_touching_bottom = self.is_touching_bottom(y2)
            #is_touching_top = self.is_touching_top(y1)
            #self.update_tracking_mask_location(current_id,mask,cam_counter,current_point,is_touching_top,is_touching_bottom)
            

            #previous_size = STORED_SIZE[closet]
            #area = w*h
            #if area < previous_size * 0.5 or area > previous_size * 1.5:
            #    return -1
                
        # elif is_new:
        #     #if y1 < LAST_Y or y1 < middle+300: #new cattle is above the last cattle, means Id 5 is in front of 4. 
        #     #    print(y1 ,' is current y1 and last y1 is: ',LAST_Y,' middle is ',middle-300)
        #     #    return -1
        #     #print('Adding new cattle')
        #     self.CATTLE_LOCAL_ID+=1
        #     self.STORED_IDS.append(self.CATTLE_LOCAL_ID)
        #     self.STORED_MISS.append(1)
        #     self.STORED_XYXY.append(current_point)
        #     current_id = str(self.CATTLE_LOCAL_ID)
        #     self.STORED_SIZE.append(w*h)

            
        return [current_id,current_point,tracking_missed_frame,is_new]

    def Centroid_Tracking_PerCamera(self,mask,w,h,cam_counter,y1,y2,frame):
        
        tracking_missed_frame = 1
        is_new = True
        current_id = -1
        
        closet = -1
        closet_iou = 0
        threshold = (w/2) if w<=h else (h/2)
        #threshold = threshold * 0.8
        threshold = threshold  # +30 is original
        current_point = self.find_mask_center(mask,cam_counter)
        least_missed_frame = 100
        if cam_counter in self.CAM_TRACKER:

            for tid,data in self.CAM_TRACKER[cam_counter].items():
                distance = self.CALCULATE_EUCLIDEAN_DISTANCE(data["tracking_xyxy"],current_point)
                
                if is_new and distance <= threshold :    
            
                    if distance < threshold/2: #distance is very close
                        is_new = False
                        closet = tid
                        #is_moving = distance > threshold / 3
                        
                        break
                    elif distance > closet_iou:
                        closet_iou = distance
                        closet = tid

            if closet!=-1 :
                #print(closet, ' I am the chosen one!!!')
                
                current_id = str(closet)
                self.CAM_TRACKER[cam_counter][closet]['tracking_xyxy'] = current_point
                self.TOTAL_MISSED_COUNT = self.CAM_TRACKER[cam_counter][closet]['tracking_missed_frame'] - 1
                #if tracking_missed_frame > 2:
                #    print(f"{current_id} has {tracking_missed_frame} missed frames")
                self.CAM_TRACKER[cam_counter][closet]['tracking_missed_frame'] = 1
                is_new = False

                #region update tracking
                is_touching_bottom = self.is_touching_bottom(y2,cam_counter)
                is_touching_top = self.is_touching_top(y1,frame)
                self.update_tracking_mask_location(current_id,mask,cam_counter,current_point,is_touching_top,is_touching_bottom)
            
        else:
            self.CAM_TRACKER[cam_counter] = {}     
        
        return [current_id,current_point,tracking_missed_frame,is_new]
        
    
    def Centroid_Tracking_v2(self,mask,w,h,cam_counter):
        
        tracking_missed_frame = 1
        is_new = True
        current_id = -1
        if len(self.STORED_IDS)>0:
            LAST_Y = self.STORED_CENTROID[-1][1] #x1y1x2y2
        closet = -1
        closet_iou = 0
        threshold = (w/2) if w<=h else (h/2)
        #threshold = threshold * 0.8
        current_point = self.find_mask_center(mask,cam_counter)
        least_missed_frame = 100
        for i in range(len(self.STORED_IDS)):
            distance = self.CALCULATE_EUCLIDEAN_DISTANCE(self.STORED_CENTROID[i],current_point)
            
            if is_new and distance <= threshold :    
                #continue
        #       is_new = False
        #       closet = i
        #       break
                if distance < threshold/2: #distance is very close
                    is_new = False
                    closet = i    
                    #is_moving = distance > threshold / 3
                    
                    break
                elif distance > closet_iou:
                    closet_iou = distance
                    closet = i

        
        if closet>-1 :
            #print(closet, ' I am the chosen one!!!')
            self.STORED_CENTROID[closet] = current_point
            current_id = str(self.STORED_IDS[closet])
            self.TOTAL_MISSED_COUNT += self.STORED_MISS[closet] - 1
            tracking_missed_frame = self.STORED_MISS[closet]-1
            #if tracking_missed_frame > 2:
            #    print(f"{current_id} has {tracking_missed_frame} missed frames")
            self.STORED_MISS[closet] = 1
            is_new = False
        
        #else: #is it old from previous tracking

   
            
        return [current_id,current_point,tracking_missed_frame,is_new]
    def addNewTracking_v2(self,mask,current_point,x1,y1,x2,y2,cam_counter,w,h):
        # tracking_id,is_touching_bottom,is_touching_top = self.find_matching_mask(mask,current_point,cam_counter,y1,y2)
        # if tracking_id != "-1":
            
        #     self.update_tracking_mask_location(tracking_id,mask,cam_counter,current_point,is_touching_top,is_touching_bottom)
        #     idx = self.STORED_IDS.index(int(tracking_id))
        #     self.STORED_XYXY[idx] = current_point

        #     self.STORED_MISS[idx] = 1
        #     print("found closed id :",tracking_id)
        #     return tracking_id

        
        
        self.CATTLE_LOCAL_ID+=1
        self.STORED_IDS.append(self.CATTLE_LOCAL_ID)
        self.STORED_MISS.append(1)
        self.STORED_XYXY.append(current_point)
        current_id = str(self.CATTLE_LOCAL_ID)
        self.STORED_SIZE.append(w*h)
        #print(f"current new id {current_id}")

        # info = MASKLOCATION(current_id,mask,cam_counter,current_point,is_touching_top,is_touching_bottom)
        # self.TRACKING_MASK_LOCATION.append(info)
        return current_id

    def addNewTracking_PerCamera(self,mask,current_point,x1,y1,x2,y2,cam_counter,w,h):
        self.CATTLE_LOCAL_ID+=1
        track_info = {
               "tracking_xyxy" : current_point,
               "tracking_missed_frame" : 1}
        self.CAM_TRACKER[cam_counter][self.CATTLE_LOCAL_ID] = track_info
        current_id = str(self.CATTLE_LOCAL_ID)
        #region update tracking
        is_touching_bottom = self.is_touching_bottom(y2,cam_counter)
        is_touching_top = self.is_touching_top(y1)
        self.update_tracking_mask_location(current_id,mask,cam_counter,current_point,is_touching_top,is_touching_bottom)
        return current_id
    def switchTrackingId(self,old_id,new_id):
        for m in self.TRACKING_MASK_LOCATION:
            if m.tracking_id == old_id:
                m.tracking_id = new_id
                break

    def addNewTracking(self,current_point,w,h):
        self.CATTLE_LOCAL_ID+=1
        self.STORED_IDS.append(self.CATTLE_LOCAL_ID)
        self.STORED_MISS.append(1)
        self.STORED_XYXY.append(current_point)
        current_id = str(self.CATTLE_LOCAL_ID)
        self.STORED_SIZE.append(w*h)
        #print(f"current new id {current_id}")
        return current_id

    def get_lowest_y(self,mask):
        Ys, Xs = np.where(mask)
        if Ys.size == 0:
            return None
        max_y = np.max(Ys)-10
        xs_at_max_y = Xs[Ys == max_y ]
        return {
            "y": max_y,                # lowest y (bottom edge)
            "x1": np.min(xs_at_max_y),  # leftmost x at max_y
            "x2": np.max(xs_at_max_y)   # rightmost x at max_y
        }
        
    def get_highest_y(self,mask):
        Ys, Xs = np.where(mask)
        if Ys.size == 0:
            return None
        min_y = np.min(Ys) + 10
        xs_at_min_y = Xs[Ys == min_y]
        return {
            "y": min_y,                # highest y (top edge)
            "x1": np.min(xs_at_min_y),  # leftmost x at min_y
            "x2": np.max(xs_at_min_y)   # rightmost x at min_y
        }
        

    def is_touching_bottom(self,y2,cam_counter, threshold=20):
        image_height = self.camera_heights[cam_counter]
        #print(image_height, " is image height")
        return abs(y2 - (image_height - 1)) <= threshold

    def is_touching_top(self,y1,frame, threshold=20):
        return y1 <= threshold

    def update_tracking_mask_location(self, tracking_id, mask, cam_counter,current_point, is_touching_top, is_touching_bottom):
        
        for m in self.TRACKING_MASK_LOCATION:
            if m.tracking_id == tracking_id:
                #print("Updating tracking id:", tracking_id, "in cam ", m.cam_counter, ' to cam ', cam_counter)

                m.cam_counter = cam_counter
                m.mask = mask
                m.is_touching_top = is_touching_top
                m.is_touching_bottom = is_touching_bottom
                m.current_point = current_point
                m.last_mask_area = np.sum(mask)
                if (not is_touching_top and not is_touching_bottom and m.mask_area < m.last_mask_area) or m.mask_area < m.last_mask_area:# or m.mask_area is null :
                    m.mask_area = m.last_mask_area

                
     
    def remove_tracking_in_other_camera(self,tracking_id, cam_counter):
        for cam_Data in self.CAM_TRACKER.values():
            if tracking_id in cam_Data:
                if cam_Data[tracking_id]['cam_counter'] != cam_counter:
                    del cam_Data[tracking_id]
                    print(f"Removed tracking {tracking_id} from camera {cam_Data[tracking_id]['cam_counter']}")
        
            

    def set_touching_cattle_count_per_cam(self):
        if m.is_touching_bottom:
            self.has_passing_cattle_camera(m.cam_counter)
            self.PASSING_CATTLE_BY_CAMERA[m.cam_counter] += 1
        if m.is_touching_top and m.cam_counter > 1:
            self.has_passing_cattle_camera(m.cam_counter-1)
            self.PASSING_CATTLE_BY_CAMERA[m.cam_counter-1] -= 1
    def has_passing_cattle_camera(self, cam_counter):
        if cam_counter not in self.PASSING_CATTLE_BY_CAMERA:
            self.PASSING_CATTLE_BY_CAMERA[cam_counter] = 0
    def get_touching_cattle_count_per_cam(self, cam_counter):
        if cam_counter in self.PASSING_CATTLE_BY_CAMERA:
            return self.PASSING_CATTLE_BY_CAMERA[cam_counter]
        return 0
    
    #compare how many pixels are overlapped from stroedX1x2 and currentX1x2
    def isOverlappedXs_original(self,first, second, threshold = 0.1):
        first_x1 = first[0]
        first_x2 = first[1]
        second_x1 = second[0]
        second_x2 = second[1]
        #print('first_x1 ',first_x1, ' first_x2 ',first_x2, ' second_x1 ',second_x1, ' second_x2 ',second_x2)
        min_length = min((first_x2 - first_x1), (second_x2 - second_x1))
        percentage = 0
        threshold = 0
        #print(first_x1, ' ',first_x2, ' ',second_x1, ' ',second_x2)
        if first_x1 < second_x1 and first_x2 > second_x2: #second is inside first
            return True,1
        elif first_x1 > second_x1 and first_x2 < second_x2 : #first is inside second
            return True,1
        elif first_x1 < second_x1 and first_x2 > second_x1: 
            percentage = self.overLappedPercentage(min_length, abs(first_x2 - second_x1))
            if percentage > threshold:
                return True,percentage
            return False, percentage
        elif second_x1 < first_x1 and second_x2 > first_x1 :
            percentage = self.overLappedPercentage(min_length, abs(second_x2 - first_x1))
            if percentage > threshold:
                return True, percentage
            return False,percentage
        return False,percentage
    def isOverlappedXs(self, first, second, threshold=0.1):
        first_x1, first_x2 = first
        second_x1, second_x2 = second

        overlap_start = max(first_x1, second_x1)
        overlap_end = min(first_x2, second_x2)
        overlap_length = max(0, overlap_end - overlap_start)

        if overlap_length <= 0:
            return False, 0

        min_length = min(first_x2 - first_x1, second_x2 - second_x1)
        percentage = self.overLappedPercentage(min_length, overlap_length)

        if percentage > threshold:
            return True, percentage
        return False, percentage

    def overLappedPercentage(self, length1, length2):
        return length2 / length1 if length1 > 0 else 0

    #divide the longer length by the shorter length
    def overLappedPercentage_original(self,length1, length2):
        
        percentage = 0
        if length1 > length2:
            percentage = length2 / length1
        else:
            percentage = length1 / length2
        #print("checking overlapped percentage length1 ",length1, ' length2 ',length2, " percentage : ",percentage)
        return percentage

    def Is_Similar(self,mask,tracking_id,x1,y1,x2,y2,frame):
        rgb_mask = np.zeros_like(frame)
        rgb_mask[mask] = frame[mask]
        box_h = max(y2-y1,x2-x1)
        crop = rgb_mask[y1:y2, x1:x2] # take crop
        crop = self.resize_image(crop,(box_h,box_h)) #then cattle image to square box
        crop = cv2.resize(crop, (self.SIZE_224,self.SIZE_224)) #resize to
        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(crop_rgb)

        #save_path = f"{self.save_dir}//{tracking_id}"
        
        #images = self.get_files_from_folder_scan_sort(save_path,10)
        images = self.get_last_X_paths(tracking_id,20)
        
        if images is None:
            return None
        images = images[5:15] # 10 images from 5th to 15th
        match_path, score = self.similarity_checker.cosine_similarity_search(images, pil_img)
        #print(f"Match: {match_path} with score {score:.2f}")
        if score > 0.7:
            print(f"✅ Match with score {score:.2f}")
            return True
        else:
            print(f"❌ Not similar with score {score:.2f}") #print(f"Deleting tracking {self.STORED_IDS[i-removed]} due to missed count {missed}")
            return False
    
    def align_centroid_per_camera(self,current_point,cam_counter):
        # Ensure mask is binary
        print(current_point)
        x,y = current_point
        y += (self.image_height * cam_counter) #adjust for multi cam
        
        return (x,y)

    #for curly version. 220 and 875 for straighten version
    def find_cattle_location(self,x1,x2):
        if x1 < 150:
            return "LEFT"
        elif x2 > 900:
            return "RIGHT"
        else:
            return "MIDDLE"

    def find_matching_mask(self, current_mask,current_point, cam_counter,tracking_id,x1,y1,x2,y2,frame, threshold=0.1):
        closed_tracking = tracking_id
        #if not is_mask_touching:
        #    print(f" skipping {tracking_id} due to not touching")
        #    return closed_tracking,False,False #if not touching, return tracking id
        is_touching_bottom = self.is_touching_bottom(y2, cam_counter)
        is_touching_top = self.is_touching_top(y1, frame)
        #if tracking_id != "-1":
            #print("tracking_id {}")
        #    return closed_tracking,is_touching_bottom,is_touching_top
        distance_threshold = 100
        check_camera = cam_counter + (0 if is_touching_bottom else - 1)
        check_camera = max(check_camera,0)
        is_overlapped = False
        cattle_position = self.find_cattle_location(x1,x2)
        mask_area_threshold = 3

        if is_touching_bottom:
            
            target_counter = cam_counter + 1
            line_info = self.get_lowest_y(current_mask)
            #print('current lowest line is :',line_info)
            if line_info is None:
                return closed_tracking,is_touching_bottom,is_touching_top

            touching_cattle_count = self.get_touching_cattle_count_per_cam(cam_counter) #if touching bottom, take current
            #is_cam_overlapped = 
            #print("touching bottom, finding in camera :", target_counter)
            for m in self.TRACKING_MASK_LOCATION:
                
                if m.cam_counter == target_counter and m.is_touching_top:
                    other_line = self.get_highest_y(m.mask)
                    if other_line is None:
                        #print(f"No highest line found for tracking ID {m.tracking_id} in camera {m.cam_counter}")
                        continue
                    isOverlapped, percentage = self.isOverlappedXs([line_info["x1"],line_info["x2"]], [other_line["x1"],other_line["x2"]],threshold)
                    distance = 1000
                    if other_line is not None and isOverlapped:
                        centroid = m.current_point
                        #print(f"{tracking_id} overlapp percentage with _tracking Id touching bot : {m.tracking_id} is {percentage} in cam {m.cam_counter}")
                        if percentage > 0.4:
                            return m.tracking_id,is_touching_bottom,is_touching_top
                    else:
                        print(f"{tracking_id} overlapp percentage with _tracking Id touching top : {m.tracking_id} is {percentage} in cam {m.cam_counter}")

        elif is_touching_top:
            target_counter = cam_counter - 1
            if target_counter < 0:
                return closed_tracking,is_touching_bottom,is_touching_top

            line_info = self.get_highest_y(current_mask)
            #print('current highest line is :',line_info)
            if line_info is None:
                print("No line found in current mask")
                return closed_tracking,is_touching_bottom,is_touching_top
            #touching_cattle_count = self.get_touching_cattle_count_per_cam(target_counter) #if touching top, take previous
            #print("touching top, finding in camera :", target_counter)
            for m in self.TRACKING_MASK_LOCATION:
                if m.cam_counter == target_counter and m.is_touching_bottom:
                    other_line = self.get_lowest_y(m.mask)
                    if other_line is None:
                        #print(f"No lowest line found for tracking ID {m.tracking_id} in camera {m.cam_counter}")
                        continue
                    isOverlapped, percentage = self.isOverlappedXs([line_info["x1"],line_info["x2"]], [other_line["x1"],other_line["x2"]],threshold)
                    distance = 1000
                    if other_line is not None and isOverlapped:
                        #print(f"{tracking_id} overlapp percentage with _tracking Id touching top : {m.tracking_id} is {percentage} in cam {m.cam_counter}")
                        if percentage > 0.4:
                            return m.tracking_id,is_touching_bottom,is_touching_top
                    else:
                        print(f"{tracking_id} overlapp percentage with _tracking Id touching top : {m.tracking_id} is {percentage} in cam {m.cam_counter}")
                    #if touching_cattle_count <= 1 and distance <distance_threshold + 100:
                        #print("Returning from touching top with cam_counter ",target_counter)
                    #    return m.tracking_id, is_touching_bottom,is_touching_top
        return closed_tracking,is_touching_bottom,is_touching_top

    
    def delete_mask_by_tracking_id(self, ids):
        #print(f"Deleting masks with tracking IDs: {ids}")
        deleted_ids = []
        for id in ids:
            self.delete_last_20_path(id)
        for m in self.TRACKING_MASK_LOCATION:
            if m.tracking_id in ids:
                deleted_ids.append(m.tracking_id)
                print(f"Deleting mask with tracking ID: {m.tracking_id}")
                self.TRACKING_MASK_LOCATION.remove(m)
        #if len(deleted_ids) != len(ids):
        #    print(f"Warning: Not all masks were deleted. Deleted IDs: {deleted_ids}, Requested IDs: {ids}")
            #for m in self.TRACKING_MASK_LOCATION:
                #print("Remaining mask with tracking ID:", m.tracking_id)


    def IncreaseMissedCount(self,tracking_ids):
        ids = []
        if(len(self.STORED_IDS)>0): 
            total_length = len(self.STORED_IDS)
        
            removed = 0
            threshold = 30
            for i in range(total_length):
                #print(i, ' ',len(STORED_MISS))
                if self.STORED_IDS[i-removed] not in tracking_ids: #didn't detected in this frame
                    self.STORED_MISS[i-removed]+=1
                    missed = self.STORED_MISS[i-removed]
                    #print(f"Increasing missed count for tracking {self.STORED_IDS[i-removed]}, missed count is now {missed} in frame {current_frame}")
                    #threshold = 5 if STORED_XYXY[i-removed][1]<= middle else 30 # if y1<= middle then above so miss count is less
                    
                    if missed>threshold: #if missed 3 frames
                        #deleted_id = self.STORED_IDS[i-removed]
                        #track_index = TRACKER.index(str(deleted_id))
                        #del TRACKER[track_index]
                        #del CLASSIFICATION_TRACKER[track_index]
                        ids.append(self.STORED_IDS[i-removed])
                        #print(f"deleting {self.STORED_IDS[i-removed]}, missed count is now {missed} in frame {current_frame}")
                        del self.STORED_MISS[i-removed]  
                        del self.STORED_XYXY[i-removed]
                        del self.STORED_SIZE[i-removed]
                        del self.STORED_IDS[i-removed]
                        removed+=1
                    #removed+=1
        if len(ids)>0:
            self.delete_mask_by_tracking_id(ids)
            #print(f"deleted ids : {ids}")

                        
    # use it when tracking conflict occurs, need to remove the previous tracking from ID not to confuse location and give incoming one as new                        
    def GetNewTrackingIDForTrackingConflict(self,tracking_id,current_point,w,h):
        
        try:
            index = self.STORED_IDS.index(tracking_id)
            del self.STORED_MISS[index]  
            del self.STORED_XYXY[index]
            del self.STORED_SIZE[index]
            del self.STORED_IDS[index]
            
        
        except:
            print(f"Error: Tracking ID {tracking_id} not found in stored IDs.")
            #new_tracking_id = self.addNewTracking(current_point,w,h)
        new_tracking_id =  self.addNewTracking(current_point,w,h)
        self.switchTrackingId(tracking_id,new_tracking_id)
        #return self.addNewTracking(current_point,w,h)
        return new_tracking_id

    def resize_image(self,input_array,new_size):
        # Open the image
        #new_size = (224,224)
        original_image = Image.fromarray(input_array)

        # Create a new image with the desired size and fill it with black pixels
        new_image = Image.new("RGB", new_size, (0, 0, 0))

        # Calculate the position to paste the original image in the center
        x_offset = (new_size[0] - original_image.width) // 2
        y_offset = (new_size[1] - original_image.height) // 2

        # Paste the original image onto the new image
        new_image.paste(original_image, (x_offset, y_offset))
        #return cv2.cvtColor(np.array(new_image),cv2.COLOR_RGB2BGR)
        return np.array(new_image)
        #bgr_array = cv2.cvtColor(numpy_array, cv2.COLOR_RGB2BGR)
        # Save the result_image
        #new_image.save(output_path)

    # def calculate_histogram(self,image, mask):
    #     # Convert image to grayscale if needed
    #     gray_image = image if len(image.shape) == 2 else cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    #     #print(f"image shape : {len(image.shape)}")
        
    #     # Ensure mask is a numpy array of type uint8
    #     mask = np.asarray(mask, dtype=np.uint8)
        
    #     # Calculate histogram using mask
    #     hist = cv2.calcHist([gray_image], [0], mask, [256], [0, 256])
    #     # print(f"hist : {hist}")

    #     # Measure histogram characteristics
    #     max_value = np.max(hist)
    #     min_value = np.min(hist)
    #     mean_value = np.mean(hist)
    #     std_dev = np.std(hist)

    #     #print(f"Max Frequency: {max_value}")
    #     #print(f"Min Frequency: {min_value}")
    #     #print(f"Mean Frequency: {round(mean_value, 2)}")
    #     #print(f"Standard Deviation: {round(std_dev, 2)}")
    #     #print("--------------------------------------------")
        
    #     return hist.flatten()

    def save_crop(self,frame,mask,x1,y1,x2,y2,save_dir,prev_id,colored_mask,is_touching,touching_pixels,is_save = True,is_small = False,predicted_id=-1,area = 1000):
        
        rgb_mask = np.zeros_like(frame)
        rgb_mask[mask] = frame[mask]
        box_h = max(y2-y1,x2-x1)
        crop = rgb_mask[y1:y2, x1:x2] # take crop
        crop = self.resize_image(crop,(box_h,box_h)) #then cattle image to square box
        crop = cv2.resize(crop, (self.SIZE_224,self.SIZE_224)) #resize to
        
        
        #straightlength = self.has_long_straight_lines(mask)
        #straightlength_box = self.check_lines_in_bbox(mask,(x1,y1,x2,y2))
        height = y2-y1
        width = x2-x1
        if (touching_pixels == True):
            self.print_border_pixels(mask,(x1,y1,x2,y2))
            #print(f"is_touching: {is_touching}, touching_pixels : {touching_pixels}")
        
        if is_save and not is_small and not is_touching: #filter out bad image

            #if x1 > 25 and x2 < self.image_width-25 and y1 > 25 and y2 < self.image_height-25 and touching_pixels == False and area > 8000:
            #    self.full_body_image[tracking_id]=crop
            base_path = str(Path(f'{save_dir}/{prev_id[0]}'))
            

            #cv2.imshow("before cropping" ,crop   )

            #crop = cv2.resize(crop, (SIZE_224,SIZE_224))
            #cv2.imshow("after cropping" ,crop )
            demo_annotated_img_save_path = Path(base_path+ '/' + f'{self.image_count}_{predicted_id}_Touching_{touching_pixels}_area{area}.jpg')
            if not os.path.exists(base_path):
                os.makedirs(base_path)
                
            cv2.imwrite(str(demo_annotated_img_save_path), crop) #save crop
            self.add_last_20_paths(prev_id[0],str(demo_annotated_img_save_path))
            self.image_count+=1
        #return crop
    def add_last_20_paths(self,tracking_id,save_path):
        #print("saving for tracking id :",tracking_id)

        if tracking_id in self.LAST_20_PATH:
            self.LAST_20_PATH[tracking_id].append(save_path)
            if len(self.LAST_20_PATH[tracking_id])>20:
                self.LAST_20_PATH[tracking_id].pop(0)  
            #print(self.LAST_20_PATH[tracking_id])

        else:
            self.LAST_20_PATH[tracking_id]= [save_path]
    
    def get_last_X_paths(self,tracking_id,take = 20):
        if type(tracking_id) == int :
            
            tracking_id = str(tracking_id)
        #print(type(tracking_id) ,' is tracking id type')
        #print("Fetching lax x paths for tracking id :",tracking_id)
        if tracking_id not in self.LAST_20_PATH:
            #print(self.LAST_20_PATH)
            print('No images ??? really???')
            return None
        length = len(self.LAST_20_PATH[tracking_id])
        start_from = length - take if length > take else 0
        response = self.LAST_20_PATH[tracking_id][start_from:]
        #print(len(response), ' is stored path for tracking_id :', tracking_id)
        return response
    
    def delete_last_20_path(self,tracking_id):
        if tracking_id in self.LAST_20_PATH:
            del self.LAST_20_PATH[tracking_id]
    def draw_mask_multiple_masks(self,colored_mask,masks,indexes):
        #boolean_mask = mask.astype(bool)
        for index in indexes:
            colored_mask[masks[index].astype(bool)] = (0, 0, 255)  # red color mask
        return colored_mask

    def diff_Time(self,start,end,process):
        return
        #print(f'{end-start} duration for {process}')
        
    def roi_no_crop(self,scale):
        # create function for image roi
        w, h, c_x, c_y = 1096, 1096, 548, 548
        x1, y1 = int(c_x - (scale * w / 2.0)), int(c_y - (0.75 * h / 2.0))
        x2, y2 = int(x1 + (scale * w)), int(y1 + (0.75 * h))

        mask_roi = cv2.cvtColor(np.zeros((1096, 1096, 1), np.uint8) * 255, cv2.COLOR_GRAY2BGR)

        mask_roi = cv2.rectangle(mask_roi, (x1, y1), (x2, y2), (255, 255, 255), -1)

        return mask_roi

    def draw_mask(self,colored_mask,mask):
        boolean_mask = mask.astype(bool)
        colored_mask[boolean_mask] = (0, 255, 255)  # red color mask
        return colored_mask
    
    def reset_duplicate_tracking_identification(self,tracking_ids):
        for tracking_id in tracking_ids:
            track_index = -1
            if tracking_id in self.TRACKER: #already have tracking record
                track_index = self.TRACKER.index(tracking_id)
            
            #batch_default = {'TOTAL_COUNT':0 , 'HOLDING' : 'Identifying','LOCATION' : self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION']}
            #batch_default = {'TOTAL_COUNT':0 , 'HOLDING' : 'Identifying','LOCATION' : location,'TOTAL_MISSED_FRAME':0,'TOTAL_DETECTION':0}
            #batch_default = {'TOTAL_COUNT':0 , 'HOLDING' : 'Identifying','LOCATION' : self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'] , }
            default = {'GT': ['Reidentifying'], 'COUNT':[0], 'IS_STABLE':False,  'HAS_MISSED_FRAME' : False, 'REIDENTIFY_MISSED_COUNT' : self.REIDENTIFY_MISSED_COUNT }
            
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = 'Identifying'
            self.CLASSIFICATION_TRACKER[track_index] = default
            
    def UpdateTrackingIDAndBatchInfo(self,old_tracking_id,location,predicted_id,w,h):
        tracking_id = self.GetNewTrackingIDForTrackingConflict(old_tracking_id,location,w,h)
        if type(tracking_id) == 'str':
            tracking_id = int(tracking_id)
        default = {'GT': [predicted_id], 'COUNT':[self.batch_size+1], 'IS_STABLE':False,  'HAS_MISSED_FRAME' : False, 'REIDENTIFY_MISSED_COUNT' : self.REIDENTIFY_MISSED_COUNT }
        self.TRACKER.append(tracking_id) #add new tracking record
        track_index = len(self.TRACKER) - 1
        #batch_default = {'TOTAL_COUNT':0 , 'HOLDING' : 'Identifying','LOCATION' : location,'TOTAL_MISSED_FRAME':0,'TOTAL_DETECTION':0}
        batch_default = {'TOTAL_COUNT':self.batch_size+1 , 'HOLDING' : predicted_id,'LOCATION' : location,'TOTAL_MISSED_FRAME':0,'TOTAL_DETECTION':0,'PREVIOUS_MAX_PREDICTED_ID':None}
        self.BATCH_CLASSIFICATION_TRACKER.append(batch_default)
        self.CLASSIFICATION_TRACKER.append(default)
        self.ALL_TIME_CLASSIFICATION_TRACKER.append(default) #to keep the whole result]
        self.TRACKING_RESULT[tracking_id] = [] #default
        self.TRACKING_RESULT[tracking_id].append(predicted_id)
        #print('Remove and insert new tracking ID from tracking conflict',self.TRACKER)
        return (predicted_id,tracking_id)
    
    def getTwoMaxIndexes(self,array):
        if(len(array) < 1):
            return -1,-1
        if len(array) < 2:
            return 0,-1
        max1 = max(array)
        max1Index = array.index(max1)
        #array.pop(max1Index)
        array[max1Index] = 0
        max2 = max(array)
        max2Index = array.index(max2)
        array[max1Index] = max1
        if max1-max2<10:
            return max1Index,max2Index
        return max1Index, -1
    
    def GetBatchPredictedId_detection_only(self, tracking_id, is_small, save_dir, missed_frame_count, location, w, h):
        batch_default = {'TOTAL_COUNT': 0, 'HOLDING': 'Identifying', 'LOCATION': location, 'TOTAL_MISSED_FRAME': 0,
                         'TOTAL_DETECTION': 0, 'PREVIOUS_MAX_PREDICTED_ID': None}
        track_index = -1
        has_missed_frame = False
        # if (tracking_id == '2' or tracking_id == 2) and missed_frame_count > 1:
        # print(f"missed frame count of tracking {tracking_id}: {missed_frame_count} ")
        if missed_frame_count > 3:
            has_missed_frame = True
            # print(f'{tracking_id} missed {missed_frame_count} frame and need to reidentify')# missed more than 5 frames then re-identify for 10 frames
        default = {'GT': [None], 'COUNT': [0], 'IS_STABLE': False, 'HAS_MISSED_FRAME': has_missed_frame,
                   'REIDENTIFY_MISSED_COUNT': self.REIDENTIFY_MISSED_COUNT}
        FORCE_IDENTIFICATION = False
        FORCE_MISSED_IDENTIFICATION = False
        HAS_TRACKING_CONFLICT = False

        IS_STABLE = False

        if tracking_id in self.TRACKER:  # already have tracking record

            track_index = self.TRACKER.index(tracking_id)

        else:
            #print(f'new tracking {tracking_id} to {self.TRACKER}')
            self.TRACKER.append(tracking_id)  # add new tracking record
            track_index = len(self.TRACKER) - 1

            self.BATCH_CLASSIFICATION_TRACKER.append(batch_default)
            self.CLASSIFICATION_TRACKER.append(default)
            self.ALL_TIME_CLASSIFICATION_TRACKER.append(default)  # to keep the whole result

            self.TRACKING_RESULT[tracking_id] = []  # default

            # print('added newq for tracking :',tracking_id)
        
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_MISSED_FRAME'] += missed_frame_count
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_DETECTION'] += 1
        # store missed_frame :
        
        return (tracking_id, tracking_id)
    
    def GetBatchPredictedId(self, tracking_id, is_small, save_dir, missed_frame_count, location, w, h):
        batch_default = {'TOTAL_COUNT': 0, 'HOLDING': 'Identifying', 'LOCATION': location, 'TOTAL_MISSED_FRAME': 0,
                         'TOTAL_DETECTION': 0, 'PREVIOUS_MAX_PREDICTED_ID': None}
        track_index = -1
        has_missed_frame = False
        # if (tracking_id == '2' or tracking_id == 2) and missed_frame_count > 1:
        # print(f"missed frame count of tracking {tracking_id}: {missed_frame_count} ")
        if missed_frame_count > 5:
            has_missed_frame = True
            # print(f'{tracking_id} missed {missed_frame_count} frame and need to reidentify')# missed more than 5 frames then re-identify for 10 frames
        default = {'GT': [None], 'COUNT': [0], 'IS_STABLE': False, 'HAS_MISSED_FRAME': has_missed_frame,
                   'REIDENTIFY_MISSED_COUNT': self.REIDENTIFY_MISSED_COUNT}
        FORCE_IDENTIFICATION = False
        FORCE_MISSED_IDENTIFICATION = False
        HAS_TRACKING_CONFLICT = False

        IS_STABLE = False

        if tracking_id in self.TRACKER:  # already have tracking record

            track_index = self.TRACKER.index(tracking_id)

        else:
            #print(f'new tracking {tracking_id} to {self.TRACKER}')
            self.TRACKER.append(tracking_id)  # add new tracking record
            track_index = len(self.TRACKER) - 1

            self.BATCH_CLASSIFICATION_TRACKER.append(batch_default)
            self.CLASSIFICATION_TRACKER.append(default)
            self.ALL_TIME_CLASSIFICATION_TRACKER.append(default)  # to keep the whole result

            self.TRACKING_RESULT[tracking_id] = []  # default

            # print('added newq for tracking :',tracking_id)
        
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_MISSED_FRAME'] += missed_frame_count
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_DETECTION'] += 1
        # store missed_frame :
        total_predictions = 0
        if not is_small:
            # print(f" tracking_id {tracking_id} is small? : {is_small}")
            # predicted_index = CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] += 1
            total_predictions = self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] 
            if self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] == True:  # keep searching if small

                self.CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT'] -= 1  # reduce by one
                FORCE_MISSED_IDENTIFICATION = self.CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT'] < 1
                if FORCE_MISSED_IDENTIFICATION:
                   # print(f"Reach limit and will do identification for missed frame of {tracking_id}")
                    # self.BATCH_CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT']  = 10 #re identify in 10
                    self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] = False  # identify in 10
        
        if has_missed_frame and total_predictions > self.batch_size and self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] != 'Identifying':
            #print(f"FORCE MISSED DETECTION FOR TRACKING {tracking_id}")

            self.CLASSIFICATION_TRACKER[track_index][
                'REIDENTIFY_MISSED_COUNT'] = self.REIDENTIFY_MISSED_COUNT  # re identify in 10
            self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] = True  # do force missed detection
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = 'Identifying'
            self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE'] = False  # if missed then not stable

            # print(self.CLASSIFICATION_TRACKER[track_index])

        IS_STABLE = self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE']
        if IS_STABLE:
            maxPredictedId = self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']
            return maxPredictedId, tracking_id

        if total_predictions > self.total_predictions and not IS_STABLE and not self.CLASSIFICATION_TRACKER[track_index][
            'HAS_MISSED_FRAME'] and not FORCE_MISSED_IDENTIFICATION:  # and if not stable
            is_moving = False
            if total_predictions % 100 == 0:  # only re calculate every 100th frame
                distance = self.CALCULATE_EUCLIDEAN_DISTANCE(location, self.BATCH_CLASSIFICATION_TRACKER[track_index][
                    'LOCATION'])  # compare two distance

                is_moving = distance > 100
                #if is_moving:
                    #print(self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'])
                    #print(location)
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'] = location

            if is_moving:  # or total_predictions %1000 == 0: #every 1000th frame
                # print("tracking id {tracking_id} is moving")
                # print("Before clearing ",self.ALL_TIME_CLASSIFICATION_TRACKER)
                #print(f're identifying tracking {tracking_id} due to moving')
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']  = 'Identifying'
                # default = {'GT': [None], 'COUNT':[0], 'IS_STABLE' : False}
                # self.CLASSIFICATION_TRACKER[track_index]
                # self.CLASSIFICATION_TRACKER[track_index] = default
                FORCE_IDENTIFICATION = True
                # print("After clearing ",self.ALL_TIME_CLASSIFICATION_TRACKER)
            elif total_predictions == 1000 and False:  # NOT RESETTING
                #print(f're identifying tracking {tracking_id} due to 1000th images')
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = 'Identifying'
                # default = {'GT': [None], 'COUNT':[0], 'IS_STABLE' : False}
                # self.CLASSIFICATION_TRACKER[track_index]
                self.CLASSIFICATION_TRACKER[track_index] = default
                FORCE_IDENTIFICATION = True
            else:
                return self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'], tracking_id
        # total_predictions = self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT']

        # if missed_frame_count >
        # print('increase count for identification')
        # print("tracking ",track_index," have ", sum(CLASSIFICATION_TRACKER[track_index]['COUNT'])," counts")

        # if missed_frame_count >
        # print(f'total predictions for {tracking_id} is : {total_predictions}')
        maxPredictedId = -1
        if total_predictions == self.batch_size or (
                total_predictions > 0 and total_predictions % self.batch_size == 0) or FORCE_IDENTIFICATION or FORCE_MISSED_IDENTIFICATION:

            # if has missed frame then show current holding -> could be identifying or id
            if self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME']:
                return self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'], tracking_id

            maxCounters = self.batch_identification(save_dir, tracking_id, FORCE_MISSED_IDENTIFICATION)
            previous_max_predicted_id = self.BATCH_CLASSIFICATION_TRACKER[track_index][
                'PREVIOUS_MAX_PREDICTED_ID']  # update previous max predicted ID
            # print(maxCounters)
            # maxPredictedId = maxCounters[0][0]

            if FORCE_MISSED_IDENTIFICATION:  # force missed identification
               # print(f"counter {maxCounters}, previous_max_predicted_id : {previous_max_predicted_id}")

                if maxCounters is None or len(maxCounters) == 0:
                    maxPredictedId = None
                else:
                    maxPredictedId = maxCounters[0][0]
                #print(
                 #   f"FORCE_MISSED_IDENTIFICATION RESULT: {maxPredictedId}, ' PREVIOUS_MAX_PREDICTED_ID : {previous_max_predicted_id}")
                if maxPredictedId is None:  # if none keep searching
                 #   print(f"FORCE MISSED DETECTION FOR TRACKING {tracking_id}")
                    self.BATCH_CLASSIFICATION_TRACKER[track_index][
                        'REIDENTIFY_MISSED_COUNT'] = self.REIDENTIFY_MISSED_COUNT  # re identify in 10
                    self.BATCH_CLASSIFICATION_TRACKER[track_index][
                        'HAS_MISSED_FRAME'] = True  # do force missed detection
                # elif previous_max_predicted_id != None and previous_max_predicted_id != 'Reidentifying' and previous_max_predicted_id != 'Identifying': #if not same then update  due to tracking conflict
                elif previous_max_predicted_id != maxPredictedId:  # different cow
                    # batch_default = {'TOTAL_COUNT':10 , 'HOLDING' : maxPredictedId,'LOCATION' : self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'] , 'HAS_MISSED_FRAME' : False, 'REIDENTIFY_MISSED_COUNT' : self.REIDENTIFY_MISSED_COUNT}
                    # default = {'GT': [maxPredictedId], 'COUNT':[10], 'IS_STABLE':False }
                    # self.BATCH_CLASSIFICATION_TRACKER[track_index] = batch_default
                    # self.CLASSIFICATION_TRACKER[track_index] = default

                    self.TRACKING_RESULT[tracking_id].append(maxPredictedId)
                    return self.UpdateTrackingIDAndBatchInfo(tracking_id, location, maxPredictedId, w, h)
                elif previous_max_predicted_id == maxPredictedId:
                    self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = maxPredictedId
                    self.BATCH_CLASSIFICATION_TRACKER[track_index]['PREVIOUS_MAX_PREDICTED_ID'] = maxPredictedId
                    return (maxPredictedId, tracking_id)

            else:
                # if len(maxCounters) == 0:
                #     maxPredictedId = None
                # region update to csv to check accuracy
                if maxCounters is not None:
                    for maxCounter in maxCounters:
                        # print(maxCounter)
                        predicted_id, predicted_count = maxCounter
                        # print(CLASSIFICATION_TRACKER[track_index])
                        ## record limited indexes
                        if predicted_id not in self.CLASSIFICATION_TRACKER[track_index]['GT']:

                            self.CLASSIFICATION_TRACKER[track_index]['GT'].append(predicted_id)
                            self.CLASSIFICATION_TRACKER[track_index]['COUNT'].append(predicted_count)
                            # print('new')
                        else:
                            predicted_index = self.CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
                            self.CLASSIFICATION_TRACKER[track_index]['COUNT'][predicted_index] += predicted_count

                        # print(self.CLASSIFICATION_TRACKER)
                        if predicted_id not in self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT']:

                            self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT'].append(predicted_id)
                            self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['COUNT'].append(predicted_count)
                            # print('new')
                        else:
                            predicted_index = self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
                            self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['COUNT'][predicted_index] += predicted_count

                maxCount = max(self.CLASSIFICATION_TRACKER[track_index]['COUNT'])
                maxCountIndex = self.CLASSIFICATION_TRACKER[track_index]['COUNT'].index(maxCount)
                maxPredictedId = self.CLASSIFICATION_TRACKER[track_index]['GT'][maxCountIndex]
                self.TRACKING_RESULT[tracking_id].append(maxPredictedId)  # add to log
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                if FORCE_IDENTIFICATION:  # due to moving
                   # print(f"Setting final ID for tracking {tracking_id} : {maxPredictedId}")
                    self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE'] = previous_max_predicted_id == maxPredictedId

                self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = maxPredictedId
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['PREVIOUS_MAX_PREDICTED_ID'] = maxPredictedId

        # else:
        #    maxPredictedId =
        
        maxPredictedId = self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']
        if tracking_id not in self.tracking_by_predicted_id:
            self.tracking_by_predicted_id[maxPredictedId] = tracking_id
        #secondMaxPredictedId = None
        # first , secondMaxIndex = self.getTwoMaxIndexes(self.CLASSIFICATION_TRACKER[track_index]['COUNT'])
        # if secondMaxIndex >0:
        #     maxPredictedId += "_"+ self.CLASSIFICATION_TRACKER[track_index]['GT'][secondMaxIndex]
        
        if maxPredictedId is None:
            maxPredictedId = 'Reidentifying'
        # if is_small:
        #    return f'{tracking_id}_{maxPredictedId}_size'
        #    #return 
        # else:
        # return (str(tracking_id)+"_"+maxPredictedId)
        return (maxPredictedId, tracking_id)
    
    def GetBatchPredictedId_TwoSimilar(self,tracking_id,is_small,save_dir,missed_frame_count,location,w,h):
        #print("tracking_id :",tracking_id, " is small :, ", is_small)
        batch_default = {'TOTAL_COUNT': 0, 'HOLDING': 'Identifying', 'LOCATION': location, 'TOTAL_MISSED_FRAME': 0,
                         'TOTAL_DETECTION': 0, 'PREVIOUS_MAX_PREDICTED_ID': None}
        track_index = -1
        has_missed_frame = False
        # if (tracking_id == '2' or tracking_id == 2) and missed_frame_count > 1:
        # print(f"missed frame count of tracking {tracking_id}: {missed_frame_count} ")
        if missed_frame_count > 5:
            has_missed_frame = True
            # print(f'{tracking_id} missed {missed_frame_count} frame and need to reidentify')# missed more than 5 frames then re-identify for 10 frames
        default = {'GT': [None], 'COUNT': [0], 'IS_STABLE': False, 'HAS_MISSED_FRAME': has_missed_frame,
                   'REIDENTIFY_MISSED_COUNT': self.REIDENTIFY_MISSED_COUNT}
        FORCE_IDENTIFICATION = False
        FORCE_MISSED_IDENTIFICATION = False
        HAS_TRACKING_CONFLICT = False

        IS_STABLE = False

        if tracking_id in self.TRACKER:  # already have tracking record

            track_index = self.TRACKER.index(tracking_id)

        else:
            #print(f'new tracking {tracking_id} to {self.TRACKER}')
            self.TRACKER.append(tracking_id)  # add new tracking record
            track_index = len(self.TRACKER) - 1

            self.BATCH_CLASSIFICATION_TRACKER.append(batch_default)
            self.CLASSIFICATION_TRACKER.append(default)
            self.ALL_TIME_CLASSIFICATION_TRACKER.append(default)  # to keep the whole result

            self.TRACKING_RESULT[tracking_id] = []  # default

            # print('added newq for tracking :',tracking_id)
        
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_MISSED_FRAME'] += missed_frame_count
        self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_DETECTION'] += 1
        # store missed_frame :
        total_predictions = 0
        if not is_small:
            # print(f" tracking_id {tracking_id} is small? : {is_small}")
            # predicted_index = CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] += 1
            total_predictions = self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] 
            if self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] == True:  # keep searching if small

                self.CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT'] -= 1  # reduce by one
                FORCE_MISSED_IDENTIFICATION = self.CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT'] < 1
                if FORCE_MISSED_IDENTIFICATION:
                    print(f"Reach limit and will do identification for missed frame of {tracking_id}")
                    # self.BATCH_CLASSIFICATION_TRACKER[track_index]['REIDENTIFY_MISSED_COUNT']  = 10 #re identify in 10
                    self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] = False  # identify in 10
        
        if has_missed_frame and total_predictions > self.batch_size and self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] != 'Identifying':
            print(f"FORCE MISSED DETECTION FOR TRACKING {tracking_id}")

            self.CLASSIFICATION_TRACKER[track_index][
                'REIDENTIFY_MISSED_COUNT'] = self.REIDENTIFY_MISSED_COUNT  # re identify in 10
            self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME'] = True  # do force missed detection
            self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = 'Identifying'
            self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE'] = False  # if missed then not stable

            # print(self.CLASSIFICATION_TRACKER[track_index])

        IS_STABLE = self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE']
        if IS_STABLE:
            maxPredictedId = self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']
            return maxPredictedId, tracking_id

        if total_predictions > 300 and not IS_STABLE and not self.CLASSIFICATION_TRACKER[track_index][
            'HAS_MISSED_FRAME'] and not FORCE_MISSED_IDENTIFICATION:  # and if not stable
            is_moving = False
            if total_predictions % 100 == 0:  # only re calculate every 100th frame
                distance = self.CALCULATE_EUCLIDEAN_DISTANCE(location, self.BATCH_CLASSIFICATION_TRACKER[track_index][
                    'LOCATION'])  # compare two distance

                is_moving = distance > 100
                if is_moving:
                    print(self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'])
                    print(location)
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'] = location

            if is_moving:  # or total_predictions %1000 == 0: #every 1000th frame
                # print("tracking id {tracking_id} is moving")
                # print("Before clearing ",self.ALL_TIME_CLASSIFICATION_TRACKER)
                print(f're identifying tracking {tracking_id} due to moving')
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']  = 'Identifying'
                # default = {'GT': [None], 'COUNT':[0], 'IS_STABLE' : False}
                # self.CLASSIFICATION_TRACKER[track_index]
                # self.CLASSIFICATION_TRACKER[track_index] = default
                FORCE_IDENTIFICATION = True
                # print("After clearing ",self.ALL_TIME_CLASSIFICATION_TRACKER)
            elif total_predictions == 1000 and False:  # NOT RESETTING
                print(f're identifying tracking {tracking_id} due to 1000th images')
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = 'Identifying'
                # default = {'GT': [None], 'COUNT':[0], 'IS_STABLE' : False}
                # self.CLASSIFICATION_TRACKER[track_index]
                self.CLASSIFICATION_TRACKER[track_index] = default
                FORCE_IDENTIFICATION = True
            else:
                return self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'], tracking_id
        # total_predictions = self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT']

        # if missed_frame_count >
        # print('increase count for identification')
        # print("tracking ",track_index," have ", sum(CLASSIFICATION_TRACKER[track_index]['COUNT'])," counts")

        # if missed_frame_count >
        # print(f'total predictions for {tracking_id} is : {total_predictions}')
        maxPredictedId = -1
        if total_predictions == self.batch_size or (
                total_predictions > 0 and total_predictions % self.batch_size == 0) or FORCE_IDENTIFICATION or FORCE_MISSED_IDENTIFICATION:

            # if has missed frame then show current holding -> could be identifying or id
            if self.CLASSIFICATION_TRACKER[track_index]['HAS_MISSED_FRAME']:
                return self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'], tracking_id

            maxCounters = self.batch_identification(save_dir, tracking_id, FORCE_MISSED_IDENTIFICATION)
            previous_max_predicted_id = self.BATCH_CLASSIFICATION_TRACKER[track_index][
                'PREVIOUS_MAX_PREDICTED_ID']  # update previous max predicted ID
            # print(maxCounters)
            # maxPredictedId = maxCounters[0][0]

            if FORCE_MISSED_IDENTIFICATION:  # force missed identification
                print(f"counter {maxCounters}, previous_max_predicted_id : {previous_max_predicted_id}")

                if len(maxCounters) == 0:
                    maxPredictedId = None
                else:
                    maxPredictedId = maxCounters[0][0]
                print(
                    f"FORCE_MISSED_IDENTIFICATION RESULT: {maxPredictedId}, ' PREVIOUS_MAX_PREDICTED_ID : {previous_max_predicted_id}")
                if maxPredictedId is None:  # if none keep searching
                    print(f"FORCE MISSED DETECTION FOR TRACKING {tracking_id}")
                    self.BATCH_CLASSIFICATION_TRACKER[track_index][
                        'REIDENTIFY_MISSED_COUNT'] = self.REIDENTIFY_MISSED_COUNT  # re identify in 10
                    self.BATCH_CLASSIFICATION_TRACKER[track_index][
                        'HAS_MISSED_FRAME'] = True  # do force missed detection
                # elif previous_max_predicted_id != None and previous_max_predicted_id != 'Reidentifying' and previous_max_predicted_id != 'Identifying': #if not same then update  due to tracking conflict
                elif previous_max_predicted_id != maxPredictedId:  # different cow
                    # batch_default = {'TOTAL_COUNT':10 , 'HOLDING' : maxPredictedId,'LOCATION' : self.BATCH_CLASSIFICATION_TRACKER[track_index]['LOCATION'] , 'HAS_MISSED_FRAME' : False, 'REIDENTIFY_MISSED_COUNT' : self.REIDENTIFY_MISSED_COUNT}
                    # default = {'GT': [maxPredictedId], 'COUNT':[10], 'IS_STABLE':False }
                    # self.BATCH_CLASSIFICATION_TRACKER[track_index] = batch_default
                    # self.CLASSIFICATION_TRACKER[track_index] = default

                    self.TRACKING_RESULT[tracking_id].append(maxPredictedId)
                    return self.UpdateTrackingIDAndBatchInfo(tracking_id, location, maxPredictedId, w, h)
                elif previous_max_predicted_id == maxPredictedId:
                    self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = maxPredictedId
                    self.BATCH_CLASSIFICATION_TRACKER[track_index]['PREVIOUS_MAX_PREDICTED_ID'] = maxPredictedId
                    return (maxPredictedId, tracking_id)

            else:
                # region update to csv to check accuracy
                for maxCounter in maxCounters:
                    # print(maxCounter)
                    predicted_id, predicted_count = maxCounter
                    # print(CLASSIFICATION_TRACKER[track_index])
                    ## record limited indexes
                    if predicted_id not in self.CLASSIFICATION_TRACKER[track_index]['GT']:

                        self.CLASSIFICATION_TRACKER[track_index]['GT'].append(predicted_id)
                        self.CLASSIFICATION_TRACKER[track_index]['COUNT'].append(predicted_count)
                        # print('new')
                    else:
                        predicted_index = self.CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
                        self.CLASSIFICATION_TRACKER[track_index]['COUNT'][predicted_index] += predicted_count

                    # print(self.CLASSIFICATION_TRACKER)
                    if predicted_id not in self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT']:

                        self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT'].append(predicted_id)
                        self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['COUNT'].append(predicted_count)
                        # print('new')
                    else:
                        predicted_index = self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
                        self.ALL_TIME_CLASSIFICATION_TRACKER[track_index]['COUNT'][predicted_index] += predicted_count

                maxCount = max(self.CLASSIFICATION_TRACKER[track_index]['COUNT'])
                maxCountIndex = self.CLASSIFICATION_TRACKER[track_index]['COUNT'].index(maxCount)
                maxPredictedId = self.CLASSIFICATION_TRACKER[track_index]['GT'][maxCountIndex]
                self.TRACKING_RESULT[tracking_id].append(maxPredictedId)  # add to log
                # self.BATCH_CLASSIFICATION_TRACKER[track_index]['TOTAL_COUNT'] = 0
                if FORCE_IDENTIFICATION:  # due to moving
                    print(f"Setting final ID for tracking {tracking_id} : {maxPredictedId}")
                    self.CLASSIFICATION_TRACKER[track_index]['IS_STABLE'] = previous_max_predicted_id == maxPredictedId

                self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING'] = maxPredictedId
                self.BATCH_CLASSIFICATION_TRACKER[track_index]['PREVIOUS_MAX_PREDICTED_ID'] = maxPredictedId

        # else:
        #    maxPredictedId =
        maxPredictedId = self.BATCH_CLASSIFICATION_TRACKER[track_index]['HOLDING']
        #secondMaxPredictedId = None
        # first , secondMaxIndex = self.getTwoMaxIndexes(self.CLASSIFICATION_TRACKER[track_index]['COUNT'])
        # if secondMaxIndex >0:
        #     maxPredictedId += "_"+ self.CLASSIFICATION_TRACKER[track_index]['GT'][secondMaxIndex]
        
        if maxPredictedId is None:
            maxPredictedId = 'Reidentifying'
        # if is_small:
        #    return f'{tracking_id}_{maxPredictedId}_size'
        #    #return 
        # else:
        # return (str(tracking_id)+"_"+maxPredictedId)
        return (maxPredictedId, tracking_id)

    def GetPredictedIDFromTracking(self,tracking_id,predicted_id,is_small):
        #print("tracking_id :",tracking_id, " is small :, ", is_small)
       
        track_index = -1
        if tracking_id in self.TRACKER: #already have tracking record
            track_index = self.TRACKER.index(tracking_id)
        else:
            self.TRACKER.append(tracking_id) #add new tracking record
            track_index = len(self.TRACKER) - 1
            default = {'GT': [predicted_id], 'COUNT':[0] , 'HOLDING' : predicted_id,'IS_PREDICTED' : False}
            self.CLASSIFICATION_TRACKER.append(default)
        total_predictions = sum(self.CLASSIFICATION_TRACKER[track_index]['COUNT']) 
        
        if not is_small:
            
            if predicted_id not in self.CLASSIFICATION_TRACKER[track_index]['GT']:
                self.CLASSIFICATION_TRACKER[track_index]['GT'].append(predicted_id)
                self.CLASSIFICATION_TRACKER[track_index]['COUNT'].append(1)
                #print('new')
            else:
                predicted_index = self.CLASSIFICATION_TRACKER[track_index]['GT'].index(predicted_id)
                self.CLASSIFICATION_TRACKER[track_index]['COUNT'][predicted_index] +=1
            
            #print('increase count for identification')
        #print("tracking ",track_index," have ", sum(CLASSIFICATION_TRACKER[track_index]['COUNT'])," counts")
        maxPredictedId = -1
        if total_predictions < 10:
            return -1
        if total_predictions == 10 or total_predictions % 10 ==0:
            maxCount = max(self.CLASSIFICATION_TRACKER[track_index]['COUNT'])
            maxCountIndex = self.CLASSIFICATION_TRACKER[track_index]['COUNT'].index(maxCount)
            maxPredictedId = self.CLASSIFICATION_TRACKER[track_index]['GT'][maxCountIndex]
            #CLASSIFICATION_TRACKER
            self.CLASSIFICATION_TRACKER[track_index]['HOLDING'] = maxPredictedId
            self.CLASSIFICATION_TRACKER[track_index]['IS_PREDICTED'] = True
            
            
        
        #print(maxPredictedI)
        #else:
        #    maxPredictedId =
        return f'{maxPredictedId}'
        #if is_small:
        #    return f'{tracking_id}_{maxPredictedId}_size'
        #    #return 
        #else:
        #    return f'{tracking_id}_{maxPredictedId}'
            
    def sortBoxAndMask(self,boxes,masks):
        # Compute width, height, and area
        widths = boxes[:, 2] - boxes[:, 0]
        heights = boxes[:, 3] - boxes[:, 1]
        areas = widths * heights

        # Add areas as a new column
        boxes_with_areas = np.hstack((boxes, areas.reshape(-1, 1)))

        # Sort boxes by area (last column) in descending order
        sorted_indices = boxes_with_areas[:, -1].argsort()[::-1]

        # Reorder the boxes and masks based on sorted indices
        sorted_boxes = boxes_with_areas[sorted_indices]
        sorted_masks = [self.keep_largest_connected_mask(masks[i]) for i in sorted_indices]
        return sorted_boxes,sorted_masks

    def isDuplicate_box(self,prev,current, cam_counter = 0):
        return False
        if(int(prev[0])==int(current[0]) and int(prev[1])==int(current[1])  and int(prev[2])==int(current[2]) and int(prev[3])==int(current[3]) ):
            return False
        x1 = max(prev[0],current[0])
        y1 = max(prev[1],current[1])
        x2 = min(prev[2],current[2])
        y2 = min(prev[3],current[3])
        w = x2 - x1
        h = y2 - y1
        if w<=0 or h<=0:
            return 0
        area = max(0,x2-x1 +1) * max(0,y2-y1+1)
        #area = w * h
        #print('total area ',area)
        prev_area = self.CalculateBoxArea(prev)
        current_area = self.CalculateBoxArea(current)
        #difference = abs(prev_area - current_area)
        #print('position 1',prev, 'position 2',current)
        #print('prev position, current position',prev_area,current_area)
        #return area / CalculateBoxArea(prev)
        #print('two box area ',prev_area,' ',current_area)
        prev_area = current_area if prev_area > current_area else prev_area
        #ratio = area / prev_area
        if cam_counter ==1:
            print(area / prev_area, " is ratio of two box")
        if area / prev_area < 0.93: #check with 95% originally
            return False #not duplicate
        return True #duplicate
        #return 
    #    iou = area / float(prev_area + current_area - area)
        
        #return iou
    def IsInsideAnotherBox(self,boxes,current_box , index, cam_counter = 0):
        if(index < 1): #first 
            return False
        contained_in_any_box = False
        #printMe =False
        until = min(index+1,len(boxes))
        #if cam_counter ==1:
        #    print(f"Checking total boxes {until} is inside another box")
        for i in range(until):
            if self.isDuplicate_box(current_box, boxes[i],cam_counter):
                #print("I am duplicated box ")
                contained_in_any_box = True
        #        printMe = True
                break
        #if printMe: 
        #    print("duplicate but :",contained_in_any_box)  
        return contained_in_any_box
    
    def isHuman(self,x1,y1,x2,y2, area):
        #if (x1 > 5 and x2<self.boundary[0] and y1 >5 and y2< self.boundary[1] and
        #already check place in isValidSize
        #print(" I am too small ")
        return area<self.SMALL_SIZE
        #return True
    def isHumanRatio(self,w,h,area):
        #override human ration if the size is big enough
        if area > self.SMALL_SIZE + 500:
            return False
        
        if w>h:
            ratio = h/w
        else :
            ratio = w/h
            
        #print(f'{ratio} is body ratio for visible cow, {w} , {h}')
        return ratio > 0.6
    
    def IsInMiddle(self,x1,y1,x2,y2):
        return x1 > 10 and y1 > 10 and x2 <self.boundary[0] + 5 and y2 < self.boundary[1]
    
    def IsTouchingBorder(self,x1,y1,x2,y2, area):
        x = x2-x1
        y = y2-y1
        if  x1<3 or y1<3 or y2>=self.boundary[1]: #or x2>=self.boundary[0]: #area < self.average_size 
            return False 
        #if x<100 or y<100 or area < SMALL_SIZE or area > BIG_SIZE  :
        
        #if area < self.SMALL_SIZE or x < 100 or y<100 or area > self.BIG_SIZE  :
            #print("size is ",x, " : " ,y)
        #    return False
        #if x<(200 * multiplier) or y<(200 * multiplier) or 
        #if area < average_size:
        #    print("size is ",x, " : " ,y)
        #    return False
        return True

    def keep_largest_connected_mask(self,mask):
        """
        Retains only the largest connected component in the mask and removes all other disconnected regions.

        Parameters:
        - mask (numpy.ndarray): Boolean mask array.

        Returns:
        - numpy.ndarray: Mask with only the largest connected component.
        """
        # Ensure mask is binary
        mask = mask.astype(np.uint8)

        # Find connected components
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)

        if num_labels <= 1:
            # No connected components found (only background)
            return np.zeros_like(mask, dtype=bool)

        # Find the largest component (excluding the background)
        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])  # Skip background (index 0)

        # Create a mask for the largest component
        largest_mask = (labels == largest_label)

        return largest_mask.astype(bool)


    def append_pair(self,tracking_id, predicted_id):
        # Check if the file exists
        global csv_file
        file_exists = os.path.exists(csv_file)
        
        # Read existing data from the file if it exists
        updated_rows = []
        tracking_found = False

        if file_exists:
            with open(csv_file, mode='r') as file:
                reader = csv.reader(file)
                for row in reader:
                    # If the tracking ID is found, append the new predicted ID
                    if row and int(row[0]) == tracking_id:
                        row.append(predicted_id)
                        tracking_found = True
                    updated_rows.append(row)

        # If tracking ID is not found, add a new row
        if not tracking_found:
            updated_rows.append([tracking_id, predicted_id])

        # Write the updated rows back to the file
        with open(csv_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerows(updated_rows)

    # Test: Add pairs dynamically
    #append_pair(1, 'AB')
    #append_pair(2, 'CD')
    #append_pair(1, 'AB')
    #append_pair(1, 'ED')
    #append_pair(2, 'ED')
    #append_pair(3, 'FF')
    #append_pair(3, 'GG')

    #print(f"Data appended to {csv_file}")
    def CoreProcess(self,boxes,masks_np,h,w,frame,cam_counter):
        #global frame_counter
        mask_count = -1
        tracked_ids = []
        colored_mask = np.zeros_like(frame)
        tracked_indexes = []
        predicted_ids = []
        original_predicted_ids = []
        counter = -1
        duplicate_ids = []
        height_threshold = 80
#        print("xxxxxxxxx ---------- xxxxxxxxxxxxxx")
        if not self.isEatingArea:
            height_threshold = 60
        
        for box in boxes:
            conver_time = datetime.now()
            x1, y1, x2, y2, _ = map(int, box)
            #self.diff_Time(conver_time,datetime.now(),' box convertion processing')
            
            counter += 1
            mask_count+=1   
            isSmall = False

            if(self.IsInsideAnotherBox(boxes,box,counter, cam_counter)):
                #print("I am skipping")
                continue #skip for this
            area = np.sum(masks_np[mask_count])

           
            touching_pixels = 0
            is_touching = False
            middle_height = 200
            #is_mask_touching = False
            if x1 <= 4 or y1 <= 4 or y2 >= self.boundary[1]: #check for 3 direction 
                touching_pixels = self.count_border_pixels(masks_np[mask_count],(x1,y1,x2,y2))
                is_touching = touching_pixels >= self.border_threshold
                #is_mask_touching = touching_pixels >= 5 # touching to border . no need to check how much touching
                if is_touching: 
                    is_touching = not self.is_showing_most_body(masks_np[mask_count], (x1,y1,x2,y2))
                #middle_height = self.calculate_mask_height(masks_np[mask_count],(x1,y1,x2,y2))
                #if is_touching:
                    #isSmall = True
                isSmall = is_touching or isSmall #middle_height < height_threshold # either touching or small height
            isSmall = self.isHuman(x1,y1,x2,y2,area) or isSmall
            #if self.IsInMiddle(x1,y1,x2,y2) and isSmall and self.isHumanRatio(x2-x1,y2-y1,area):
            #    continue
            
            
            if True :#HOKKAIDO_ROI(x1,y1,x2,y2,h,w):  #only cow class
                #Centroid_Tracking_PerCamera
                #prev_id = self.Centroid_Tracking(masks_np[mask_count],x2-x1,y2-y1,cam_counter,y1,y2)
                prev_id = self.Centroid_Tracking(masks_np[mask_count],x2-x1,y2-y1,cam_counter,y1,y2)                
                #predicted_id = en.predict_by_efficientnet(crop)
                #(prev_id,is_moving) =IOU_Tracking(x1,y1,x2,y2)
                #is_moving = True
                if(prev_id==-1): #skip cattle when prev_id // filter id is -1
                    #print("tracking skipping")
                    continue
                
                
                is_touching = False
                middle_height = 200
               
                isInMiddle = self.IsInMiddle(x1,y1,x2,y2)
                if isInMiddle and area < 6000:
                    #print("I am middle : and small with area", area )
                    continue
                is_new = prev_id[3] # check is_new
                
                tracking_id,is_touching_bottom,is_touching_top = self.find_matching_mask(masks_np[mask_count],prev_id[1],cam_counter,prev_id[0],x1,y1,x2,y2,frame)
                if tracking_id != -1:
                    if int(tracking_id) in self.STORED_IDS:
                        #print(f" tracking id {tracking_id} is in camera {cam_counter}")
                        #if not is_touching_top and not is_touching_bottom: # not touching then tracking can only be in one place
                        #    self.remove_tracking_in_other_camera(tracking_id,cam_counter) #remove tracking in other camera
                        idx = self.STORED_IDS.index(int(tracking_id))
                        self.update_tracking_mask_location(tracking_id,masks_np[mask_count],cam_counter,prev_id[1],is_touching_top,is_touching_bottom)
                        
                        self.STORED_XYXY[idx] = prev_id[1]

                        self.STORED_MISS[idx] = 1
                        #print("found closed id :",tracking_id)
                        #prev_id[2] = 
                        prev_id[0] = tracking_id
                        is_new = False
                    else:
                        #print(f"Unable to find tracking id {tracking_id} in stored Ids")
                        self.delete_mask_by_tracking_id([str(tracking_id)])
                    #return tracking_id
                    #if int(tracking_id) == 9:
                    #    print(f"tracking id {tracking_id} is in camera {cam_counter} and is touching top {is_touching_top} and bottom {is_touching_bottom}")

                if is_new:
                    # tracking_id,is_touching_bottom,is_touching_top = self.find_matching_mask(masks_np[mask_count],prev_id[1],cam_counter,y1,y2)
                    # if tracking_id != "-1":
                        
                    #     self.update_tracking_mask_location(tracking_id,masks_np[mask_count],cam_counter,prev_id[1],is_touching_top,is_touching_bottom)
                    #     idx = self.STORED_IDS.index(int(tracking_id))
                    #     self.STORED_XYXY[idx] = prev_id[1]

                    #     self.STORED_MISS[idx] = 1
                    #     print("found closed id :",tracking_id)
                    #     #prev_id[2] = 
                    #     prev_id[0] = tracking_id
                    #     #return tracking_id
                    if isSmall and self.isHumanRatio(x2-x1,y2-y1,area) : #prioritize the size first
                        #print(" I am too small haha")
                        #frame = draw_bounding_box(frame, (x1, y1, x2, y2), f'{area}',4,color=(218,200,30))
                        continue
                    elif isInMiddle and isSmall:
                        #print(" I am too small Middle")
                        #frame = draw_bounding_box(frame, (x1, y1, x2, y2), f'{area}',4,color=(218,200,30))
                        continue
                        #if isSmallSize #or is_touching or self.isHumanRatio(x2-x1,y2-y1,area): #if new is touching then wait it
                        #    continue
                    #print(prev_id[0],' area :',area, 'SMALL SIZE :', self.SMALL_SIZE )
                    else:
                        
                    #prev_id[0] = self.addNewTracking(prev_id[1],w,h) #if not touching then insert itssssssssss
                        prev_id[0] = self.addNewTracking_v2(masks_np[mask_count],prev_id[1],x1,y1,x2,y2,cam_counter,w,h)
                        print("new tracking id : ",prev_id[0], " is touching  top ", is_touching_top, " is touching bottom ", is_touching_bottom, " camera ", cam_counter)
                        info = MASKLOCATION(prev_id[0],masks_np[mask_count],cam_counter,prev_id[1],is_touching_top,is_touching_bottom)
                        self.TRACKING_MASK_LOCATION.append(info)
                        #print("new tracking id :",prev_id[0])
                #if self.isValidSize(x1, y1, x2, y2,area):
                #    print(f'Tracking {prev_id[0]},  is small due to having {area} < {self.SMALL_SIZE}')
                #    isSmall = True  
                #else :
                    #check for missing frame count and detect again
                        
                #print(prev_id[0],'s mask area is ',area)
                # if len(self.cow_filter_size)<100 and not isSmall and not is_touching and self.isEatingArea:
                #     self.cow_filter_size.append(area)
                #     if len(self.cow_filter_size) == 100:
                #         self.average_size = (sum(self.cow_filter_size)/100)
                #         self.SMALL_SIZE = self.average_size * 0.7 #BACK TO 0.6 AND ONLY APPLY TO NEW
                #         if self.isEatingArea and self.SMALL_SIZE < 11000:
                #             self.SMALL_SIZE = 11000
                #         elif not self.isEatingArea and self.SMALL_SIZE < 7000:
                #             self.SMALL_SIZE = 7000
                #         self.BIG_SIZE = self.average_size * 1.6
                #         #self.average_size = self.average * 0.9 #some time only partial stuck at bottom
                
                #     if self.RESET_AVERAGE_SIZE_AFTER < 1:
                #         self.cow_filter_size.clear()
                #         self.RESET_AVERAGE_SIZE_AFTER = 1100
                    
                #     self.RESET_AVERAGE_SIZE_AFTER -=1
                
                #predicted_id =
                #max_freq_pos = 120
                #with lock:
                #colored_mask = self.draw_mask(colored_mask,masks_np[mask_count])
                
                #if False and not FilterSize(y1,y2,h25,poly_area,h75,max_freq_pos) :
                    #await draw_bounding_box(frame, (x1, y1, x2, y2), f's{poly_area}',4,color=(218,200,30))
                #    continue
                #print("I was skipped")
                
                
                tracked_ids.append(int(prev_id[0]))
                tracked_indexes.append(mask_count)
                
                missed_frame_count = int(prev_id[2])
                
                #print(f'tracking id {prev_id[0]} is  small ? :::::: {isSmall}')
                #asyncio.create_task(save_crop(frame,mask,x1,y1,x2,y2,save_dir,prev_id,colored_mask))
                #crop = 
                #region partial detection
                    
                #if not isSmall :
                #isSmall = False # to know actual result
                #partial detection
                #predicted_id,tracking_id = self.GetBatchPredictedId_detection_only(int(prev_id[0]),isSmall,self.save_dir,missed_frame_count,prev_id[1],w,h)
                #predicted_id
                predicted_id,tracking_id = self.GetBatchPredictedId(int(prev_id[0]),isSmall,self.save_dir,missed_frame_count,prev_id[1],w,h)
                
                #prev_id[0] = tracking_id
                self.save_crop(frame,masks_np[mask_count],x1,y1,x2,y2,self.save_dir,prev_id,colored_mask,is_touching,touching_pixels,is_save=True,is_small=isSmall,predicted_id= predicted_id,area=area)
                if predicted_id != '' and predicted_id!='Identifying' and predicted_id in predicted_ids and False:
                    duplicate_ids.append(predicted_id)
                
                predicted_ids.append(predicted_id)
                
                    
        #reset ideentification for tracking                    
        # if len(duplicate_ids) > 0:
            
        #     duplicate_indexes = []
        #     print(duplicate_ids,' duplicate ids')
        #     for duplicate_id in duplicate_ids:
        #         duplicate_indexes.append([index for index, value in enumerate(predicted_ids) if value == duplicate_id])
        #     duplicate_tracking_indexes = []
        #     for duplicaters in duplicate_indexes: 
        #         #print(duplicate_indexes, ' is duplicate ids and processing on ',duplicaters )
        #         for index in duplicaters:
        #             predicted_ids[index] = 'Identifying'
        #             duplicate_tracking_indexes.append(tracked_ids[index])
        #     #print("Predited ID duplicate on tracking: " , duplicate_tracking_indexes)
        #     self.reset_duplicate_tracking_identification(duplicate_tracking_indexes)
            
        #if HAS_COW:
            #predicted_ids = self.parallel_identification(data_to_pred) 
        #print("xxxxxxxxx ---------- xxxxxxxxxxxxxx")
        response_model = CoreProcessResponseModel(cam_counter, boxes ,tracked_indexes, tracked_ids, predicted_ids, original_predicted_ids, colored_mask)
        return response_model
        
    def stack_image_from_bottom_to_top(self,image_list):
        """
        Stack images from bottom to top with a maximum height for each image.
        """
        #604 603 602 601 format
        
        stacked_image = None
        for img in image_list:
            #img = cv2.imread(imgpath)
             
            if stacked_image is None:
                #stacked_image = cv2.resize(img, (self.image_width, self.image_height))
                stacked_image = img
            else:
                # Resize the new image to fit the width of the stacked image

                
                # resized_img = cv2.resize(img, (self.image_width, self.image_height))
                
                # Stack the images vertically
                stacked_image = np.vstack((stacked_image, img))
        #print(stacked_image.shape, ' image shape')
        return stacked_image

    def dispose_model(self,predictor):
    # If using GPU, free up CUDA memory
    
        del predictor  # Remove reference
        torch.cuda.empty_cache()  # Clear the CUDA memory
        print("Model disposed and GPU memory released.")

    def get_duplicate_indexes(self,array):
        index_map = defaultdict(list)
        for i, val in enumerate(array):
            index_map[val].append(i)

        # Only keep values with duplicates
        duplicates = {val: idxs for val, idxs in index_map.items() if len(idxs) > 1}
        return duplicates
    def get_max_start_time_per_video(self,video_list):
        max_start_time = 0
        start_times = []
        print(video_list)

        for video in video_list:
            start_time = int(video.split('\\')[-1].split("-")[1])
            #print(start_time)
            start_times.append(start_time)
            if start_time > max_start_time:
                max_start_time = start_time
        
        #print(start_times)
        #extrack max start time from every start time
        start_times = [max_start_time - x for x in start_times]

        return start_times

    
    # ---------- STITCH IMAGES ----------
   

    def vertical_stitch_cut_both_original(self,img1, img2, img3,
                                cut_top1=0.0, cut_bot1=0.0,
                                cut_top2=0.0, cut_bot2=0.0,
                                cut_top3=0.0, cut_bot3=0.0):
        def crop(img, top_pct, bot_pct):
            h = img.shape[0]
            top = int(h * top_pct)
            bottom = int(h * (1 - bot_pct))
            return img[top:bottom]

        cropped1 = crop(img1, cut_top1, cut_bot1)
        cropped2 = crop(img2, cut_top2, cut_bot2)
        cropped3 = crop(img3, cut_top3, cut_bot3)

        #cropped3 = self.draw_quarter_circles(cropped3,130)

        stitched = np.vstack([cropped1, cropped2])
        return cropped3, stitched

        
    def stack_tetris_crops(self,img1,img2, overlap = 20):
        h1, w = img1.shape[:2]
        h2 = img2.shape[0]

        # Calculate the final height
        final_height = h1 + h2 - overlap

        # Initialize result image
        result = np.zeros((final_height, w, 3), dtype=np.uint8)

        # Copy top part of img1 (non-overlapping)
        result[:h1 - overlap] = img1[:h1 - overlap]

        # Blend the overlapping region
        overlap_img1 = img1[h1 - overlap:]
        overlap_img2 = img2[:overlap]

        # Create a mask where img2 is non-black
        mask = np.any(overlap_img2 != [0, 0, 0], axis=2)

        # Start with overlap from img1
        blended_overlap = overlap_img1.copy()

        # Overwrite black pixels in img1 with non-black from img2
        blended_overlap[mask] = overlap_img2[mask]

        # Place blended overlap
        result[h1 - overlap:h1] = blended_overlap

        # Copy the remaining part of img2
        result[h1:] = img2[overlap:]

        return result
    def add_black_blocks(self,image, xy_positions, block_size=(510, 20)):
        """
        Draws black blocks at specified (x, y) positions on the image.

        Args:
            image (np.ndarray): Input image (BGR).
            xy_positions (list of tuples): List of (x, y) top-left corners.
            block_size (tuple): Width and height of the block (w, h).

        Returns:
            np.ndarray: Modified image with black blocks drawn.
        """
        
        h_img, w_img = image.shape[:2]
        w_blk, h_blk = block_size

        for x, y in xy_positions:
            x_end = min(x + w_blk, w_img)
            y_end = min(y + h_blk, h_img)
            x_start = max(x, 0)
            y_start = min(max(y, 0),h_img-h_blk)
            #print('Drawing from ({}, {}) to ({}, {})'.format(x_start, y_start, x_end, y_end))
            image[y_start:y_end, x_start:x_end] = (0, 0, 0)  # Black in BGR

        return image
    def vertical_stitch_cut_both(self,img1, img2, img3,
                                cut_top1=0.0, cut_bot1=0.0,
                                cut_top2=0.0, cut_bot2=0.0,
                                cut_top3=0.0, cut_bot3=0.0):
        def crop(img, top_pct, bot_pct):
            h = img.shape[0]
            top = int(h * top_pct)
            bottom = int(h * (1 - bot_pct))
            return img[top:bottom]

        cropped1 = crop(img1, cut_top1, cut_bot1)
        cropped2 = crop(img2, cut_top2, cut_bot2)
        cropped3 = crop(img3, cut_top3, cut_bot3)
        #cropped1 = apply_tetris_crop(cropped1, x1=160, x2=670, length=20, shape_type='T')
        #cropped2 = apply_tetris_crop(cropped2, x1=160, x2=670, length=20, shape_type='U')
        overlap = 30
        positions_1_1 = [(0,2000)]
        block_size_1_1 = (145,overlap)
        positions_1_2 = [(670,2000)]
        block_size_1_2 = (1152-670, overlap)  


        #cropped1 = add_black_blocks(cropped1, positions_1_1, block_size_1_1)
        cropped1 = self.add_black_blocks(cropped1, positions_1_2, block_size_1_2)

        positions_2 = [(0,0)]
        block_size_2 = (670, overlap) 
        cropped2 = self.add_black_blocks(cropped2, positions_2, block_size_2)

        image = cv2.imwrite("D:/stitched_tetris_cropped1.png",cropped1)
        image = cv2.imwrite("D:/stitched_tetris_cropped2.png",cropped2)

        top_2 = self.stack_tetris_crops(cropped1, cropped2, overlap=overlap)

        #stitched = np.vstack([cropped1, cropped2, cropped3])
        #stitched = np.vstack([top_2, cropped3])
        return cropped3, top_2

    # ---------- TILE IMAGE VERTICALLY ----------
    def get_vertical_tiles(self, image, tile_height=800, overlap=200):
        h, w = image.shape[:2]
        tiles = []
        y_starts = list(range(0, h, tile_height - overlap))

        for y in y_starts:
            y_end = min(y + tile_height, h)
            tile = image[y:y_end]
            tiles.append((tile, y))
        return tiles

    # ---------- RUN DETECTION ON TILES ----------
    def detect_tiles(self, stitched_img):
        all_boxes = []
        all_scores = []
        all_classes = []
        all_masks = []

        for tile, y_offset in self.get_vertical_tiles(stitched_img, tile_height=800, overlap=200):
            outputs = self.predictor(tile)
            instances = outputs["instances"].to("cpu")

            if len(instances) == 0:
                continue

            boxes = instances.pred_boxes.tensor.numpy()
            boxes[:, [1, 3]] += y_offset  # shift Y

            masks = instances.pred_masks.numpy()
            shifted_masks = np.zeros((masks.shape[0], stitched_img.shape[0], stitched_img.shape[1]), dtype=bool)
            for i in range(masks.shape[0]):
                mask = masks[i]
                h_tile = mask.shape[0]
                shifted_masks[i, y_offset:y_offset + h_tile, :] = mask  # shift vertically

            all_boxes.append(boxes)
            all_scores.append(instances.scores.numpy())
            all_classes.append(instances.pred_classes.numpy())
            all_masks.append(shifted_masks)

        return all_boxes, all_scores, all_classes, all_masks


    # ---------- MERGE DETECTIONS WITH NMS ----------
    def apply_nms(self, all_boxes, all_scores, all_classes, all_masks, iou_threshold=0.5):
        if not all_boxes:
            return [], [], [], []

        boxes = np.vstack(all_boxes)
        scores = np.hstack(all_scores)
        classes = np.hstack(all_classes)
        masks = np.vstack(all_masks)  # shape: [N, H, W]

        boxes_tensor = torch.tensor(boxes)
        scores_tensor = torch.tensor(scores)

        keep = nms(boxes_tensor, scores_tensor, iou_threshold)

        final_boxes = boxes_tensor[keep].numpy()
        final_scores = scores_tensor[keep].numpy()
        final_classes = classes[keep.numpy()]
        final_masks = masks[keep.numpy()]

        return final_boxes, final_scores, final_classes, final_masks


    # ---------- DRAW DETECTIONS ----------
    def draw_detections(self, image, boxes, scores, classes):
        image_copy = image.copy()
        for box, score, cls in zip(boxes, scores, classes):
            x1, y1, x2, y2 = box.astype(int)
            cv2.rectangle(image_copy, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label = f"ID:{cls} {score:.2f}"
            cv2.putText(image_copy, label, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        return image_copy

    def draw_quarter_circles(self,image, radius=100):
        h, w = image.shape[:2]

        # Top-left corner (from corner to center - upper left quarter)
        cv2.ellipse(
            image,
            center=(0, 0),
            axes=(int(radius/2), int(radius/2)),
            angle=0,
            startAngle=0,
            endAngle=90,
            color=(0, 0, 0),
            thickness=-1
        )

        # Top-right corner (upper right quarter)
        cv2.ellipse(
            image,
            center=(w, 0),
            axes=(radius, radius),
            angle=0,
            startAngle=90,
            endAngle=180,
            color=(0, 0, 0),
            thickness=-1
        )

        return image
    def get_Tracking_To_Merge(self,data):
        # Dictionary to hold indices of each value
        value_indices = defaultdict(list)
        #print("")
        # Populate the dictionary
        for idx, val in enumerate(data):
            value_indices[val].append(idx)

        # Extract duplicates and their indexes
        duplicates = {val: idxs for val, idxs in value_indices.items() if len(idxs) > 1}
        return duplicates
    def combine_boxes(self,box1, box2):
        x1 = min(box1[0], box2[0])
        y1 = min(box1[1], box2[1])
        x2 = max(box1[2], box2[2])
        y2 = max(box1[3], box2[3])
        return [x1, y1, x2, y2]

    def batch_frames(self,data, file_name, is_show_image, cap= None, tracking_cap = None, isFile = False, isCam = False):
        video_file = data[0]
        print(video_file, " I am video file")
        start_timer_adjustment = self.get_max_start_time_per_video(data)
        global images
        #if isCam:
        #    video_file = video_dir
        #else:
        #    video_file = f"{video_dir}" if isFile else f"{video_dir}\\{file_name}"
        
        self.boundary = self.save_width-7,self.save_height-7
        #print(video_file,' I am video file')
        # video_readers = cv2.VideoCapture(video_file)
        video_readers = []
        apply_correction_on = {}
        #distortion_keys,distortion_settings
        video_counter = 0
        for vdo in data:
            
            video_reader = cv2.VideoCapture(vdo)
            

            video_readers.append(video_reader)
            video_counter += 1
        
        is_save_video = cap is not None
        is_save_tracking_video = tracking_cap is not None
        #is_save_image = True
        print('are we saving video? ',is_save_video)
        print('are we saving tracking video? ',is_save_tracking_video)
        print(f'current values small size {self.SMALL_SIZE}')

        #w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
        #h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        middle = self.save_height * 0.25
        h25= self.save_height*0.25
        h75 = self.save_height*0.75
        #h90 = h*0.9

               
      
        skip_frame = self.frame_skip_count # math.ceil(CAP_PROP_FPS / 5)-1   
        frame_count = skip_frame+1
        
        ret = True
        skip_minute,skip_second = 4,45
        manual_skip = 0 #(skip_minute * 60 + skip_second)  * 20
        while ret:
            #print("are we reading?")
            start_time = datetime.now()
            total_time_start = start_time
            images = []
            vid_counter = 0
            for video in video_readers:
                ret, frame = video.read()
                if manual_skip > 0:
                    continue
                if not ret :#or manual_skip > 0:
                    break
                images.append(cv2.resize(frame,self.resolution))
                vid_counter += 1
            if manual_skip > 0:
                print(manual_skip," skipping frames")
                manual_skip -= 1
                continue
            #if not ret:
            #    return 
            if(len(images) != len(video_readers)):
                print("All images len not match, len is ", len(images), " , videos len is ", len(video_readers))
                return
            self.camera_heights.clear()
            if len(images) > 0:
                frame, stitched = self.vertical_stitch_cut_both(
                            images[0], images[1], images[2],
                            #cut_top1=0.0,  cut_bot1=1-0.8833,  # Keep top of Cam1, cut 18% bottom
                            #cut_top2=0.4208, cut_bot2=1-0.7916,  # Cut 18% top and 21% bottom of Cam2
                            #cut_top3=0.19, cut_bot3=0.0    # Cut 21% top of Cam3, keep full bottom
                            cut_top1=0.0,  cut_bot1=1-0.9093,  # Keep top of Cam1, cut 18% bottom
                            cut_top2=0.4208, cut_bot2=1-0.7916,  # Cut 18% top and 21% bottom of Cam2
                            cut_top3=0.19, cut_bot3=0.0    # Cut 21% top of Cam3, keep full bottom
                        )
                images = [stitched, frame]
                self.camera_heights.append(stitched.shape[0])
                self.camera_heights.append(frame.shape[0])
                #print(" after stitching , all images len is ", len(images))
                
            
            frame_count+=1
            if frame_count<=skip_frame: #read every 4th frame
                continue
            frame_count = 0
            batch_outputs = list(self.stupid_detector(images))
            
            # print('predicted outputs:',batch_outputs)
            cam_counter = 0
            all_frame_infos = []
            #print("xxxxxxxxx ---Core start------- xxxxxxxxxxxxxx")
            for outputs in batch_outputs:  #per frame
                
                #print(len(outputs), ' is the length of outputs')
                frame = images[cam_counter]
                start_time = datetime.now()
                #print('how come')
                instances = outputs["instances"].to("cpu")
                #boxes = instances.pred_boxes.tensor.numpy()
                #classes = instances.pred_classes.numpy()
                #masks_np = instances.pred_masks.numpy()
                boxes,masks_np = self.sortBoxAndMask(instances.pred_boxes.tensor.numpy(),instances.pred_masks.numpy())
                mask_count = 0
                h, w, _ = frame.shape
                #print(outputs)
                HAS_COW = False
                
                
                
                start_time = datetime.now()
                
                coreProcessResponse = (self.CoreProcess(boxes,masks_np,h,w,frame, cam_counter))
                
                all_frame_infos.append(coreProcessResponse)
                
                
                self.diff_Time(start_time,datetime.now(),' tracking and validation process')
                
                
                #frame = cv2.addWeighted(images[cam_counter], 1.0, coreProcessResponse.colored_mask, 0.5, 0)
                images[cam_counter] = frame
                cam_counter+=1

            #update touching cattle count per cam for each camera
            #self.set_touching_cattle_count_per_cam()
            #print("xxxxxxxxx ---Core end------- xxxxxxxxxxxxxx")
            all_tracking_ids = [item for sublist in all_frame_infos for item in sublist.tracked_ids]
            all_duplicate_idexes = self.get_duplicate_indexes([item for sublist in all_frame_infos for item in sublist.predicted_ids])
            
            tracking_to_merge = self.get_Tracking_To_Merge(all_tracking_ids)
            #if len(tracking_to_merge) > 0:
            #    print(f"Need to merge {tracking_to_merge}")
            
            all_duplicate_tracking_ids = []
            #print(f"{all_tracking_ids} is all tracking id")
            #duplicate across all images
            
            boxes_to_merge = defaultdict(list)
            for duplicate in all_duplicate_idexes:
                if duplicate == "Identifying" or duplicate== "Reidentifying":
                    continue
                #print(duplicate, " duplicate value" )
                duplicate_tracking_ids = [all_tracking_ids[i] for i in all_duplicate_idexes[duplicate]]
                #print(duplicate_tracking_ids, " duplicate tracking ids")
                #is same tracking across images?
                is_same = all(val == duplicate_tracking_ids[0] for val in duplicate_tracking_ids)
                #no same tracking id across images, reset all
                if not is_same:
                    for id in duplicate_tracking_ids:
                        all_duplicate_tracking_ids.append(id)
                #print(is_same," is same ", list(set(duplicate_tracking_ids)))

            #Duplilcate across all images
            if(len(all_duplicate_tracking_ids) > 0):
                #print("resetting duplilcate ids from tracking : ",all_duplicate_tracking_ids   )
                self.reset_duplicate_tracking_identification(all_duplicate_tracking_ids)
            #all_duplicate_tracking_ids.clear()
            
            for frame_info in all_frame_infos:
                #print(frame_info.tracked_ids, ' is the tracked ids')
                mask_count = 0
                for index in frame_info.tracked_indexes:
                    #print("I am cow")
                    x1, y1, x2, y2, area = map(int, frame_info.boxes[index])
                    
                    #print(tracked_ids, '  are predicted ids')
                    #await(draw_bounding_box(original_frame,(x1,y1,x2,y2),original_predicted_ids[mask_count],font_scale=1)) # draw with predicted id
                    #label = f'{str(tracked_ids[mask_count])}_{str(predicted_ids[mask_count])}'
                    #if isCam:
                    tracking_id = frame_info.tracked_ids[mask_count]
                    label = 'Identifying' if frame_info.tracked_ids[mask_count] in all_duplicate_tracking_ids else f'{str(frame_info.predicted_ids[mask_count])}'
                    if tracking_id in tracking_to_merge.keys():
                        sum_y = 0
                        if frame_info.cam_counter > 0:
                            sum_y = self.camera_heights[frame_info.cam_counter-1]
                        boxes_to_merge[tracking_id] += [[x1,y1+sum_y,x2,y2+sum_y],label]
                    else:
                        self.draw_bounding_box(images[frame_info.cam_counter],(x1,y1,x2,y2),label,str(frame_info.tracked_ids[mask_count]),font_scale=1) # draw with predicted id
                    # if is_save_tracking_video:
                    #     self.draw_bounding_box(original_frame,(x1,y1,x2,y2),label,tracked_ids[mask_count],font_scale=1) # draw with predicted id
                    #self.draw_mask(images[frame_info.cam_counter],frame_info.masks_np[mask_count])
                    mask_count+=1
            
            
            self.IncreaseMissedCount(all_tracking_ids)
            start_time = datetime.now()
            #frame = cv2.resize(frame, (1088, 1088))
            stacked_image = self.stack_image_from_bottom_to_top(images)
            for key,box_to_merge in boxes_to_merge.items():
                print(key, box_to_merge)
                x1,y1,x2,y2 = self.combine_boxes(box_to_merge[0],box_to_merge[2])
                label = box_to_merge[1]
                try:
                    if y2-y1 > 650:
                        print(" too big to merge")
                        x1,y1,x2,y2 = box_to_merge[0]
                        self.draw_bounding_box(stacked_image,(x1,y1,x2,y2),box_to_merge[1],str(key),font_scale=1) # draw with predicted id
                        x1,y1,x2,y2 = box_to_merge[2]
                        self.draw_bounding_box(stacked_image,(x1,y1,x2,y2),box_to_merge[3],str(key),font_scale=1) # draw with predicted id
                    #else:
                    else:
                        self.draw_bounding_box(stacked_image,(x1,y1,x2,y2),label,str(key),font_scale=1) # draw with predicted id
                except:
                    print("Error in merging boxes for key:", key, "with box_to_merge:", box_to_merge)
            if stacked_image is None:
                print("Stacked image is None")
                break
            if is_save_video:
                #print('I am saving the footage of moon')
                #showVideo = cv2.resize(frame, (1080, 1080), interpolation=cv2.INTER_AREA)
                #print("saving stacked_iamge in shape", stacked_image.shape)
                cap.write(stacked_image)


            if is_save_tracking_video:
                #print('I am saving the footage of venus')
                tracking_cap.write(original_frame)
                
            self.diff_Time(start_time,datetime.now(),' file writing process')    # resizedImage = cv2.resize(v.get_image(), (1000, 1000)) 
            
            start_time = datetime.now()

            if is_show_image and False:
                #cv2.imshow("kunneppu",cv2.resize(frame, (1080, 1080)) )
                imshow_size = 576 , int((self.image_height/2)*len(data))
                #print(imshow_size, " Imshow size")
                cv2.imshow("NASA doing centroid tracking on the cattle (detectron 2)", cv2.resize(stacked_image, (imshow_size)))
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    cv2.destroyAllWindows()
                    pass
                    #self.dispose_model(predictor)
                    return -1
            self.diff_Time(start_time,datetime.now(),' image showing process')

            self.diff_Time(total_time_start,datetime.now(),' the whole process')
        #self.dispose_model(predictor)
        return    

           
    

    def find_mask_center(self,mask,cam_counter):
        # Ensure mask is binary
        if not np.issubdtype(mask.dtype, np.bool_):
            mask = mask > 0
        
        # Find the coordinates of non-zero pixels
        y_coords, x_coords = np.where(mask)
        
        # Compute the center of the mask
        center_x = np.mean(x_coords)
        center_y = np.mean(y_coords)
         #region Adjust height for multi cams
        center_y = center_y +  (0 if cam_counter == 0 else self.camera_heights[cam_counter-1])
        
        #if cam_counter > 0:
            #print("Center Y is adjusted for camera ", cam_counter, " to ", center_y)
        return (center_x, center_y)


    def drawArrayOverImage(self,img):
        
        # Parameters for drawing
        spacing = 0  # spacing between rectangles
        rect_height = 70   # height of each rectangle
        rect_width =  100 # width of each rectangle

        # Offset to position the arrays correctly on the image
        x_offset = 50
        y_offset = 2500

        # Font settings
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        font_color = (0, 255, 255)
        font_thickness = 2
        labels = ['ID','MISS']
        

        # Iterate over arrays and draw them
        for i in range(2):
            array = self.STORED_IDS if i == 0 else self.STORED_MISS
        
        #print('damn I am too kind!!! ',(zip(STORED_IDS, STORED_MISS)))
        #for i, (array) in (zip(STORED_IDS, STORED_MISS)):
            # Draw the label
            #print("I am drawling array")
            
            label_size = cv2.getTextSize(labels[i], font, font_scale, font_thickness)[0]
            label_x = x_offset - label_size[0] - 10
            label_y = y_offset + i * (rect_height + spacing) + rect_height // 2 + label_size[1] // 2
            
            # Draw the label
            cv2.putText(img, labels[i], (label_x, label_y), font, font_scale, font_color, font_thickness, cv2.LINE_AA)
            #for k in range(2):
            for j in range (26):
                
                value = '-'
                if j< len(array):
                    value = array[j]
                
                #y_dy_offset = y_offset if j<13 else y_offset * 2    
                #print('I am drawing Array value')
                top_left = (x_offset + j * (rect_width + spacing), y_offset + i * (rect_height + spacing))
                bottom_right = (top_left[0] + rect_width, top_left[1] + rect_height)
                
                # Draw rectangle
                cv2.rectangle(img, top_left, bottom_right, (255, 255, 255), 5)
                
                # Put the text inside the rectangle
                text_position = (top_left[0] + rect_width // 2 - 20, top_left[1] + rect_height // 2 + 10)
                cv2.putText(img, str(value), text_position, font, font_scale, font_color, font_thickness, cv2.LINE_AA)
        return img

    def clearData(self):
        #global CATTLE_LOCAL_ID
        print("Clearing data")
        self.TRACKER.clear()  #To store tracking index to use in classifier
        self.CLASSIFICATION_TRACKER.clear()#[{'GT': [1, 2, 3], 'COUNT': [10, 2, 5]}] retrieve Index , GT is ground truth, COUNT is number of apperance
        self.BATCH_CLASSIFICATION_TRACKER.clear()
        self.ALL_TIME_CLASSIFICATION_TRACKER.clear()
        self.TRACKING_RESULT.clear()
        self.STORED_IDS.clear()   # move it up for each folder
        self.STORED_MISS.clear()
        self.STORED_XYXY.clear()
        self.CATTLE_LOCAL_ID = 0  
        self.cow_filter_size.clear()
        self.TRACKING_MASK_LOCATION.clear()
        self.LAST_20_PATH.clear()
        self.PASSING_CATTLE_BY_CAMERA.clear()
        

    def save_final_pdf(self,save_dir,file_name):
        #global TRACKER_clone
        #global ALL_TIME_CLASSIFICATION_TRACKER_clone
        #global class_names_clone
        #TRACKER_clone = self.TRACKER
        #ALL_TIME_CLASSIFICATION_TRACKER_clone = self.ALL_TIME_CLASSIFICATION_TRACKER
        print("Saving finalize pdf :",save_dir)
        default_data = ["ID"]
        final_result = []
        missed_frame_data = []
        default_missed_frame_data = ['Tracking','Total','Missed']
        missed_frame_data.append(default_missed_frame_data)
        #class_names_clone = self.class_names
        for i in (self.class_names):
            default_data.append(self.class_names[i])
        #print(default_data)
        
        final_result.append(default_data)
        #print(f"===> Stored data  Tracker {self.TRACKER}")
        #print(f"===> Stored data  ALL_TIME_CLASSIFICATION_TRACKER {self.ALL_TIME_CLASSIFICATION_TRACKER}")
        for index in range(len(self.TRACKER)):
            try:

                tracking_id = self.TRACKER[index]
                if(trackig_id is None):
                    continue
                track_index = len(self.TRACKER) - 1
                #dataset = list(range(100,141))
                dataset = self.ALL_TIME_CLASSIFICATION_TRACKER[index]
                stx = [int(tracking_id)]
                
                missed_data = []
                missed_data.append(tracking_id)
                missed_data.append(self.BATCH_CLASSIFICATION_TRACKER[index]['TOTAL_DETECTION'])
                missed_data.append(self.BATCH_CLASSIFICATION_TRACKER[index]['TOTAL_MISSED_FRAME'])
                missed_frame_data.append(missed_data)
                
                #batch_default = {'TOTAL_COUNT':0 , 'HOLDING' : 'Identifying','LOCATION' : location,'TOTAL_MISSED_FRAME':0,'TOTAL_DETECTION':0}
                #self.BATCH_CLASSIFICATION_TRACKER.append(batch_default))
                for idx in (self.class_names):
                    id = str(self.class_names[idx])
            #        print(id)
                    if id not in self.ALL_TIME_CLASSIFICATION_TRACKER[index]['GT']:
                        stx.append(0)    
                        #print(id,0)
                    else:
                        predicted_index = self.ALL_TIME_CLASSIFICATION_TRACKER[index]['GT'].index(id)
                        predicted_count = self.ALL_TIME_CLASSIFICATION_TRACKER[index]['COUNT'][predicted_index]
                        #print(id,predicted_count)
                        stx.append(predicted_count)
                    
                final_result.append(stx)   
            except Exception as e:
                print(f"Error processing index {index}: {e}")
        df = pd.DataFrame(final_result)
        df.to_csv(f"{save_dir}\\{file_name}.csv", index=False,header=False)
        missed_df = pd.DataFrame(missed_frame_data)
        missed_df.to_csv(f"{save_dir}\\missed_count.csv", index=False,header=False)
        self.save_accuracy_like_program(save_dir)
        
    def save_accuracy_like_program(self,save_dir,file_name='program_accuracy'):
        final_result = []
        #final_result.append("ID")
        for key in (self.TRACKING_RESULT):
            finalData = []
            finalData.append(key)
            #finalData.append(result_list[key])
            
            for value in self.TRACKING_RESULT[key]:
                finalData.append(value)
            final_result.append(finalData)
        print(final_result)   
        df = pd.DataFrame(final_result)
        df.to_csv(f"{save_dir}\\{file_name}.csv", index=False,header=False)
        
        #self.save_missed_frame_count(save_dir)
    
        

    def has_long_straight_lines(self,mask, min_length=100, frame_height=None):
        #print(mask, ' is mask')
        height = frame_height
        # Check for horizontal lines
        has_horizontal = False
        horizontal_y2 = None
        for y, row in enumerate(mask):
            line_starts = np.where(np.diff(np.concatenate(([False], row))))[0]
            line_ends = np.where(np.diff(np.concatenate((row, [False]))))[0]
            
            # Ensure line_starts and line_ends have the same shape
            min_len = min(len(line_starts), len(line_ends))
            line_lengths = line_ends[:min_len] - line_starts[:min_len]
            
            if np.any(line_lengths >= min_length):
                has_horizontal = True
                horizontal_y2 = y
                break

        # Check for vertical lines
        has_vertical = False
        vertical_y2 = None
        for x, col in enumerate(mask.T):
            line_starts = np.where(np.diff(np.concatenate(([False], col))))[0]
            line_ends = np.where(np.diff(np.concatenate((col, [False]))))[0]
            
            # Ensure line_starts and line_ends have the same shape
            min_len = min(len(line_starts), len(line_ends))
            line_lengths = line_ends[:min_len] - line_starts[:min_len]
            
            if np.any(line_lengths >= min_length):
                has_vertical = True
                if line_ends[-1] == height:  # Check if the line extends to the bottom of the mask
                    vertical_y2 = frame_height - 1
                else:
                    vertical_y2 = line_ends[-1] - 1
                break

        return has_horizontal, has_vertical#, horizontal_y2, vertical_y2
    
    def check_lines_in_bbox(self,mask, bbox, min_length=50, threshold=50):
        x1, y1, x2, y2 = bbox
        height, width = mask.shape

        top_hits = 0
        left_hits = 0
        bottom_hits = 0

        # Check for horizontal lines
        for y in range(y1, y2 + 1):
            if y >= height:
                break
            row = mask[y, x1:x2+1]
            line_starts = np.where(np.diff(np.concatenate(([False], row))))[0]
            line_ends = np.where(np.diff(np.concatenate((row, [False]))))[0]
            
            min_len = min(len(line_starts), len(line_ends))
            line_lengths = line_ends[:min_len] - line_starts[:min_len]
            
            long_lines = line_lengths >= min_length
            if np.any(long_lines):
                if y == y1:
                    top_hits += np.sum(line_lengths[long_lines])
                elif y == y2:
                    bottom_hits += np.sum(line_lengths[long_lines])

        # Check for vertical lines
        for x in range(x1, x2 + 1):
            if x >= width:
                break
            col = mask[y1:y2+1, x]
            line_starts = np.where(np.diff(np.concatenate(([False], col))))[0]
            line_ends = np.where(np.diff(np.concatenate((col, [False]))))[0]
            
            min_len = min(len(line_starts), len(line_ends))
            line_lengths = line_ends[:min_len] - line_starts[:min_len]
            
            long_lines = line_lengths >= min_length
            if np.any(long_lines):
                if x == x1:
                    left_hits += np.sum(line_lengths[long_lines])

        return top_hits >= threshold, left_hits >= threshold, bottom_hits >= threshold

    def check_orientation(self,box,ratio_threshold=1.5):
        
        width = box[2] - box[0]
        height = box[3] - box[1]

        if height > width * ratio_threshold:
            return 100 #"vertical"
        elif width > height * ratio_threshold:
            return 150#"horizontal"
        else:
            return 80# "diagonal"
    def is_showing_most_body(self,mask, bbox):
        centroid_threshold = self.check_orientation(bbox)
        #print(centroid_threshold, ' is centroid threshold')
        
        (x,y) = self.find_mask_center(mask,0)
        #print(x , ' : ',  y, ' is centroid')
        if x > centroid_threshold and x < self.image_width-centroid_threshold and y > centroid_threshold and y < self.image_height-centroid_threshold:
            return True
        return False
            
    
    def count_border_pixels(self,mask, bbox, border_margin=5):
        
        threshold = self.border_threshold
        """
        Count the number of pixels touching each border of the image and determine if it exceeds the threshold.
        
        :param mask: 2D numpy array (boolean mask of the object)
        :param bbox: tuple of (x1, y1, x2, y2) representing the bounding box
        :param threshold: int, the minimum number of pixels to consider the object as touching the border
        :param border_margin: int, margin to consider when checking if object touches the border
        :return: tuple of (is_partial, counts), where is_partial is a boolean and counts is a dict of pixel counts for each border
        """
        if not isinstance(mask, np.ndarray) or mask.ndim != 2:
            raise ValueError("Input mask must be a 2D numpy array")

        height, width = mask.shape
        x1, y1, x2, y2 = bbox

        # Check if the bounding box is partially outside the mask
       
        # Count pixels touching each border
        left_count = np.sum(mask[y1:y2+1, max(0, x1-border_margin):min(width, x1+border_margin)])
        if self.isEatingArea:
            right_count = 0
        else:
            right_count = np.sum(mask[y1:y2+1, max(0, x2-border_margin):min(width, x2+border_margin)])
        top_count = np.sum(mask[max(0, y1-border_margin):min(height, y1+border_margin), x1:x2+1])
        bottom_count = np.sum(mask[max(0, y2-border_margin):min(height, y2+border_margin), x1:x2+1])

        # counts = {
        #     'left': int(left_count),
        #     'right': int(right_count),
        #     'top': int(top_count),
        #     'bottom': int(bottom_count)
        # }

        # # Check if any border count exceeds the threshold
        # is_partial = any(count >= threshold for count in counts.values())
        #if is_partial:
        #    exceeding_counts = {value for key, value in counts.items() if value >= threshold}
        #    print(exceeding_counts)
        
        counts = [
             int(left_count),
             int(right_count),
             int(top_count),
              int(bottom_count)
         ]
        maxTouching =  max(counts) #, counts
        #if maxTouching == True:
        #    print(counts)
        return maxTouching
    
    def print_border_pixels(self,mask, bbox, border_margin=3):
        
        threshold = self.border_threshold
        """
        Count the number of pixels touching each border of the image and determine if it exceeds the threshold.
        
        :param mask: 2D numpy array (boolean mask of the object)
        :param bbox: tuple of (x1, y1, x2, y2) representing the bounding box
        :param threshold: int, the minimum number of pixels to consider the object as touching the border
        :param border_margin: int, margin to consider when checking if object touches the border
        :return: tuple of (is_partial, counts), where is_partial is a boolean and counts is a dict of pixel counts for each border
        """
        if not isinstance(mask, np.ndarray) or mask.ndim != 2:
            raise ValueError("Input mask must be a 2D numpy array")

        height, width = mask.shape
        x1, y1, x2, y2 = bbox

        # Check if the bounding box is partially outside the mask
        #if x1 < 0 or y1 < 0 or x2 >= width or y2 >= height:
        #    return True #, {'left': 0, 'right': 0, 'top': 0, 'bottom': 0}

        # Count pixels touching each border
        left_count = np.sum(mask[y1:y2+1, max(0, x1-border_margin):min(width, x1+border_margin)])
        if self.isEatingArea:
            right_count = 0
        else:
            right_count = np.sum(mask[y1:y2+1, max(0, x2-border_margin):min(width, x2+border_margin)])
        top_count = np.sum(mask[max(0, y1-border_margin):min(height, y1+border_margin), x1:x2+1])
        bottom_count = np.sum(mask[max(0, y2-border_margin):min(height, y2+border_margin), x1:x2+1])

        # counts = {
        #     'left': int(left_count),
        #     'right': int(right_count),
        #     'top': int(top_count),
        #     'bottom': int(bottom_count)
        # }

        # # Check if any border count exceeds the threshold
        # is_partial = any(count >= threshold for count in counts.values())
        #if is_partial:
        #    exceeding_counts = {value for key, value in counts.items() if value >= threshold}
        #    print(exceeding_counts)
        
        counts = [
             int(left_count),
             int(right_count),
             int(top_count),
              int(bottom_count)
         ]
      
        print("Printing border pixels for touching_pixels getting True : ", counts)
        
    
    def calculate_mask_height(self, mask, bbox):
        """
        Calculate the height of the mask along the longer dimension of the bounding box.
        
        :param mask: 2D numpy array (boolean mask)
        :param bbox: tuple of (x1, y1, x2, y2) representing the bounding box
        :return: tuple of (height, plane), where height is the calculated height and plane is either 'X' or 'Y'
        """
        if not isinstance(mask, np.ndarray) or mask.ndim != 2:
            raise ValueError("Input must be a 2D numpy array")

        x1, y1, x2, y2 = bbox
        bbox_width = x2 - x1
        bbox_height = y2 - y1

        # Determine whether to use X or Y plane based on the longer dimension of the bbox
        if bbox_width < bbox_height:
            plane = 'X'
            center = (y1 + y2) // 2
            row = mask[center, x1:x2+1]
            true_indices = np.where(row)[0]
            height = len(true_indices)
        else:
            plane = 'Y'
            center = (x1 + x2) // 2
            column = mask[y1:y2+1, center]
            true_indices = np.where(column)[0]
            height = len(true_indices)

        return height #height, plane
    
    def mask_height_from_center(self,mask):
        if not isinstance(mask, np.ndarray) or mask.ndim != 2:
            raise ValueError("Input must be a 2D numpy array")

        # Find the center of the mask
        height, width = mask.shape
        center_y = height // 2

        # Find the first True value from top and bottom
        true_rows = np.where(mask.any(axis=1))[0]
        if len(true_rows) == 0:
            return 0  # No True values in the mask

        top = true_rows[0]
        bottom = true_rows[-1]

        # Calculate the height from the center
        height_above = max(center_y - top, 0)
        height_below = max(bottom - center_y, 0)

        # Return the maximum height from center
        return max(height_above, height_below)
    
    def get_all_videos(self,path: str, extensions=None):
        if extensions is None:
            extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm'}
        
        path = Path(path)
        video_files = [str(p) for p in path.rglob("*") if p.suffix.lower() in extensions]
        return video_files
    def begin_process(self,cam_datas,isCam,camera_number=None,lane = "Left"):
        cap = None
        tracking_cap = None
        
        number_of_cams = len(cam_datas)
        number_of_videos = len(cam_datas[0])
        print()
        for idx in range(number_of_videos): 
            
        
            video_paths = []
            for i in range(number_of_cams):
                #videos.append(cam_datas[i][idx])
                #print(cam_datas[i][idx], ' is video file')
                cam_videos = self.get_all_videos(cam_datas[i][idx])
                #for vid in cam_videos:
                #    print(vid)
                video_paths.append(cam_videos)
            print(len(video_paths), ' is video len and len video_paths[0] ', len(video_paths[0]))
            video_dir = cam_datas[0][0]
            #print(video_dir)
            csv_file = "tracking_prediction.csv"
            DATE = video_dir.split("\\")[-2]+'_'+video_dir.split("\\")[-1]
            channel = video_dir.split("\\")[-3] #hourly
            model_name = self.model_path.split("\\")[-1]
            project = f'G:/Output/runs/{self.FARMNAME}_identification/KNO_Merge_and_Batch_identification/July Report 2025/Sumiyoshi-44/{model_name}/testris_{channel}'
            name = f'{DATE}_part ' #weight path and iou threshold
            
            self.save_dir = Helper.increment_path(Path(project) / name,mkdir=True)
            csv_file = f'{self.save_dir}\\{csv_file}'
            #print(save_dir)
            #region Video data
            save_vid_name = video_dir.split("\\")[-1]+"_identification" # open this when running multiple videos
            tracking_vid_name = video_dir.split("\\")[-1]+"_tracking" # open this when running multiple videos

            isFile = False
            if video_dir.endswith(".mp4") or video_dir.endswith(".mkv"):
                isFile = True
                save_vid_name=  video_dir.split('\\')[-1].replace('.mkv','_track').replace('.mp4','_track')  #open this when running single video
                tracking_vid_name=  video_dir.split("\\")[-1].replace('.mkv','_cow_only_track').replace('.mp4','_cow_only_track')  #open this when running single video


            #print(save_vid_name,' is file :'  ,isFile)
            save_vid_path = str(Path(os.path.join(self.save_dir, save_vid_name)).with_suffix('.mp4'))
            tracking_vid_path = str(Path(os.path.join(self.save_dir, tracking_vid_name)).with_suffix('.mp4'))
            #continue
            is_quit = 1
            #cap =  None #: cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 6, (1920,1080))
            self.boundary = self.save_width-10,self.save_height-10
            
            
            self.resolution = (1024, 768) # with number of camera because of stacking
            self.isEatingArea = False
            self.save_width,self.save_height = (1024,1575)
            self.boundary = self.save_width,self.save_height
            self.average_size = 30000
            self.SMALL_SIZE = 6000 # need to use it in clear data
            self.border_threshold = 80
            self.BIG_SIZE = 700000
                
                
            #tracking_cap = cv2.VideoWriter(tracking_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (int(resolution[0] * multiplier), int(resolution[1] *multiplier)))
            #cap = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (int(resolution[0] * multiplier), int(resolution[1] *multiplier)))
            
            tracking_cap = None #cv2.VideoWriter(tracking_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (self.save_width,self.save_height))
            #print("save_width and save_height", self.save_width,self.save_height)
            cap = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (self.save_width,self.save_height))
            
            #endregion Video data
            csv_main_file_path = str(self.save_dir) + "\\main_csv.csv"
            #await generate_frames(video_dir,entry,True,cap,cow_only_cap)
            
            print(save_vid_path)
            
            for i in range(len(video_paths[0])):
                videos = []
                for cam_no in range(number_of_cams):
                    
                    print(video_paths[cam_no][i], ' is video file')
                    videos.append(video_paths[cam_no][i])
                    #continue
                    #print(video_file, ' is video file')
                    #cap = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (self.save_width,self.save_height))
                    #tracking_cap = cv2.VideoWriter(tracking_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 5, (self.save_width,self.save_height))
                    #print(video_dir,' I am video dir')
                
                #if isCam:
                #    video_file = video_dir
                #else:
                #    video_file = f"{video_dir}" if isFile else f"{video_dir}\\{file_name}"
                is_quit = self.batch_frames(videos, lane, True, cap, tracking_cap)                                  
            #return
            #self.clearData()
            #task.wait()
            if is_quit == -1:
                break
            #else: 
                #return
        self.save_final_pdf(self.save_dir,'finalized_pdf')
        if cap is not None:
            cap.release()
        if tracking_cap is not None:
            tracking_cap.release()
        self.clearData()
        #cow_only_cap.release()
        print('done saving everything')
        if cap is not None:
            cap.release()
        if tracking_cap is not None:
            tracking_cap.release()
        
        cv2.destroyAllWindows()


    def setup_distortion_settings():
        setting5_52 = CorrectDistortedImage().createSetting(k1=0.0,k2=0.0,k3=0.0,p1=0.0,p2=-0.005,focal_length_x=1.0,focal_length_y=1.0,center_x=0.5,center_y=1.0,scale=1.01,aspect=1.0)    
        setting5_53 = CorrectDistortedImage().createSetting(k1=0.8,k2=-1.0,k3=-0.98,p1=-0.036,p2=-0.029,focal_length_x=1.47,focal_length_y=1.47,center_x=0.36,center_y=0.7,scale=1.03,aspect=1.0)    
        setting5_54 = CorrectDistortedImage().createSetting(k1=0.64,k2=0.86,k3=1.0,p1=-0.039,p2=-0.012,focal_length_x=1.1,focal_length_y=1.59,center_x=0.51,center_y=0.82,scale=1.01,aspect=1.0)
        setting6_03 = CorrectDistortedImage().createSetting(k1=-0.12,k2=0.17,k3=-0.05,p1=0.007,p2=0.008,focal_length_x=1.39,focal_length_y=0.64,center_x=0.53,center_y=0.53,scale=1.01,aspect=1.0)
        setting003 = CorrectDistortedImage().createSetting(k1=1.0,k2=0.0,k3=0.0,p1=-0.042,p2=-0.069,focal_length_x=4.71,focal_length_y=1.72,center_x=0.72,center_y=0.73,scale=1.02,aspect=1.64)
        setting004 = CorrectDistortedImage().createSetting(k1=-0.03,k2=0.29,k3=-0.14,p1=-0.041,p2=-0.0,focal_length_x=0.85,focal_length_y=0.81,center_x=0.51,center_y=0.54,scale=1.01,aspect=1.0)
        settings = {}
        keys = ["5-52","5-53","5-54","-6-03"," 003"," 004"]
        settings['5-52'] = setting5_52
        settings['5-53'] = setting5_53
        settings['5-54'] = setting5_54
        settings['-6-03'] = setting6_03
        settings[' 003'] = setting003
        settings[' 004'] = setting004
        return keys,settings

    def setup_overlapping_settings():
        overlapping_settings = {}
        overlapping_settings['5-52'] = True
        overlapping_settings['5-53'] = True
        overlapping_settings['5-54'] = False
        #settings['-6-03'] = setting6_03
        #settings[' 003'] = setting003
        #settings[' 004'] = setting004
        return overlapping_settings
    #region parameters
    #region Tracker
    STORED_IDS =[]
    STORED_MISS = []
    STORED_XYXY = []
    STORED_CENTROID = []
    STORED_SIZE = []
    CATTLE_LOCAL_ID = 0
    IOU_TH = 0.5
    #endregion Tracker
    ############################ tracking parameters #########################################

    #region variables
    HOKKAIDO = "HOKKAIDO"
    HONKAWA = "HONKAWA"
    SUMIYOSHI = "SUMIYOSHI"
    YOSHII = "YOSHII"
    KUNNEPPU = "KUNNEPPU"
    FARMNAME = KUNNEPPU
    LAST_Y=0
    prevId_record = []
    #endregion variables
    TOTAL_MISSED_COUNT = 0
    middle = 0

    thread_pool = []

    # Predict with the model
    TRACKING = TRACKING_TYPE.CENTROID # IOU, Box, Centroid
    WEIGHT_PATH = '' # train for Hokkaido path, train 3 for Sumiyoshi
    
    DATE = '2024-08-07'
    alpha = 1.2  # Contrast control (1.0-3.0)
    beta = 20   # Brightness control (0-100)
    isShowBrightness = True #show brightness False show original
    multiplier = 1
    removeFilterAfter = 0
    cow_filter_size = []
    average_size = 30000
    SMALL_SIZE = 6000 # need to use it in clear data
    border_threshold = 80
    BIG_SIZE = 700000
    RESET_AVERAGE_SIZE_AFTER = 50 # first is 50, rest is is
    batch_size = 5 #identification batch
    total_predictions = 50
    identification_batch_size = batch_size 
    frame_skip_count = 3 if FARMNAME == SUMIYOSHI else 5  #skip 3 frame read 1 frame, 5fps in 20 fps video

    image_width = 1152
    image_height = 648

    camera_heights = []
    isEatingArea = False
    #dataset = f"E:/Nyi Zaw Aung/SuLarbMon/815_CowDataChecking/{DATE}/{HOUR}/20230317_061029_BB92.mkv" #2023-11-27 09 and 18 cows

    TRACKER = []  #To store tracking index to use in classifier
    CLASSIFICATION_TRACKER = [] #[{'GT': [1, 2, 3], 'COUNT': [10, 2, 5]}] retrieve Index , GT is ground truth, COUNT is number of apperance
    ALL_TIME_CLASSIFICATION_TRACKER = []
    BATCH_CLASSIFICATION_TRACKER = []
    TRACKING_RESULT = {} # [TrackingID, [values from each batch]]
    REIDENTIFY_MISSED_COUNT = 5 # re identify if there are missed frame
    manual_cow_count = 0
    image_count = 1
    cow_count = 1
    # base_path = "E:\\sumiyoshi\\ch006\\2024-06-10\\15\\"
    isWholeDay = False
    is_quit = 1
    #resolution = (1920,1080)
    resolution = (1152,648)
    save_width,save_height = resolution
    boundary = save_width,save_height
    save_dir = ""
    SIZE_224 = 224
    image_count = 1
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("using ",device)

    #region added attributes
    TRACKING_MASK_LOCATION = []
   
    #model_path ="models\\December 2024\\Sumiyoshi_dec16_v2_human_2024_10000_iters_v1" #Sumiyoshi_dec16_v3_human_2024_10000_iters_v6
    model_path ="models\\Base-sidelane June 2025\\Base_rtx8000_June_15000_v2" #Sumiyoshi_dec16_v3_human_2024_10000_iters_v6
    print("initializing under main")
    similarity_checker = CheckSimilarity()
    class_names = []
    #FeaturesExtractor\\Sumiyoshi-40-24Oct-2024\\label-sumiyoshi-40-v2-50epochs-pretrained_27oct2024.json oct 24 #final    
    #with open2 is('FeaturesExtractor\\Sumiyoshi-38-ep50\\label-sumiyoshi-38-v1-50epochs-pretrained_27oct2024.json', 'r') as f:
    predictor_path = "identification_models_KNP\\KNP_ConvNeXtV2_fcmae_ft_in22k_in1k_v_RTX8000_1"
    with open(f'{predictor_path}\\class_mappings.json', 'r') as f:
        class_names = json.load(f)
    #print(class_names)
    #identification_model_path = f"{predictor_path}\\efficientnetv2s_cattle.pth" #99.70% weight_epoch_140_weights
    
    identification_model_path = f"{predictor_path}\\model_best.pth" 
    model = _load_model(len(class_names),identification_model_path,device)
    #model = _load_model(len(class_names), identification_model_path, device)
    stupid_detector = load_detector()
    #model = models.efficientnet_v2_m(weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1)
    #num_ftrs = model.classifier[1].in_features
    #model.classifier[1] = nn.Linear(num_ftrs, len(class_names))  # Update to your number of classes
    #FeaturesExtractor\\Sumiyoshi-40-31Oct-2024\\efficientv2-m-sumiyoshi-40-v1-50epochs-pretrained_31oct2024.pth
    #model.load_state_dict(torch.load("FeaturesExtractor\\Sumiyoshi-38-ep50\\efficientv2-m-sumiyoshi-38-v1-50epochs-pretrained_27oct2024.pth")) # 23 oct update 4 browns
    #model.load_state_dict(torch.load("Identification models\\efficientv2-m-sumiyoshi-40-v1-50epochs-pretrained_31oct2024.pth")) # 31 oct update 4 browns
    LAST_20_PATH = {}
     
    PASSING_CATTLE_BY_CAMERA = {} ## camera counter [0]=2 cattle 
    #print("total class is : ", len(class_names))
    #endregion parameters
    barrel_distortion_corrector = CorrectDistortedImage()

    TRACKER_clone = []
    ALL_TIME_CLASSIFICATION_TRACKER_clone = []
    
    #distortion_keys,distortion_settings = setup_distortion_settings()
    # distortion_keys,distortion_settings = setup_distortion_settings_straight()
    # camera_overlapping_setting = setup_overlapping_settings()
    camera_lists = {}

    tracking_by_predicted_id = {}
    colors = [(255,0,0),(0,255,0),(0,0,255),(0,0,0)]

    CAM_TRACKER = {} 

    full_body_image = {}

def draw_bounding_boxes_out(image, boxes):
    for box in boxes:
        x1, y1, x2, y2 = box
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
    return image

def draw_masks_out(image, masks):
    try:
        for mask in masks:
            color = (0, 255, 0)  # Green color for the mask
            print(image.shape, ' is image')
            print(mask.shape, ' is mask')
            image[mask] = color
    except Exception as ex:
        cv2.imshow("Error in mask", image)
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    return image
if __name__ == '__main__':
    

    begin_identification = CATTLE_IDENTIFICATION()
    inputs = [
        
    #F:\Nyi Zaw Aung\815_CowDataChecking\KNP\Ch004_Camera 004_42\2025-04-16 10/11/12/13
    # [
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\00"],
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\00"],
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\00"],
    #         ],
    #         [
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\01"],
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\01"],
    #             ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\01"],
    #         ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\02"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\02"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\02"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\03"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\03"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\03"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\04"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\04"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\04"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\05"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\05"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\05"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\06"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\06"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\06"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\07"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\07"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\07"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\08"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\08"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\08"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\09"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\09"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\09"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\10"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\10"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\10"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\1125"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\1125"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\1125"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\12"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\12"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\12"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\13"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\13"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\13"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\14"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\14"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\14"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\15"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\15"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\15"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\16"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\16"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\16"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\17"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\17"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\17"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\18"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\18"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\18"],
            # ],
            # [
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\19"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\19"],
            #     ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\19"],
            # ],
            [
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\20"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\20"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\20"],
            ],
            [
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\21"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\21"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\21"],
            ],
            [
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\22"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\22"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\22"],
            ],
            [
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch005_Camera 005_42\\2025-04-17\\23"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch002_Camera 002_42\\2025-04-17\\23"],
                ["G:\\Nyi Zaw Aung\\815_CowDataChecking\\KNP\\Standard_Format\\ALL_Channel_One_Day\\Ch001_Camera 001_42\\2025-04-17\\23"],
            ],
            
    ]
    
 
    for allCams in inputs:
        begin_identification.begin_process(allCams,isCam= False,camera_number=None)
        try:
            print("Beginning processing now")
            #begin_identification.begin_process(input_data,isCam= False,camera_number=None)
        
        except Exception as ex:
            print("We got a problem in running sir, ",ex)
    print("Finished")
    
    